---
title: "ARIMA modeling"
author: "Maurício Collaça"
date: "11 de agosto de 2018"
output: 
  html_document: 
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
options(width = 100)
```

# Time Series Regression Models

Regression: $Y_i = \beta X_i + \epsilon_i$ where $\epsilon_i$ is **white noise**

## White Noise (WN)

* independent normals with common variance  
* is basic building block of time series

## AutoRegression (AR)

$X_t = \phi X_{t-1} + \epsilon_t$ ($\epsilon_t$ is **white noise**)

Tipically time series data are correlated and assuming error are not correlated may lead to a bad forecast.  One way to overcome this problems is to use a Moving Average for the errors.

## Moving Average (MA)

$\epsilon_t = W_t + \theta W_{t-1}%$ ($W_t$ is **white noise**)

Errors $\epsilon_t$ at time t are correlated to the errors $\epsilon_{t-1}$ at time t-1 because the both have a "$W_{t-1}$"

## Autoregressive + Moving Average (ARMA)

$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$

Putting these two together is a model with autocorrelation and autocorrelated errors.

# Stationarity and Nonstationarity

In the context of time series, it is stationary when it is “stable”, meaning:

* the mean is constant over time (no trend)  
* the correlation structure remains constant over time

Stationarity means we can use simple averaging to estimate $\bar X$ correlation.

Pairs can be used to estimate correlation on different lags: (x1,x2)(x2,x3)... for lag 1.

This works because relation between contiguous values of the series remains the same overtime.

Similarly (x1,x3)(x2,x4)... for lag 2.

Stationary example: Southern Oscillation Index

Reasonable to assume stationary, but perhaps some slight trend.

```{r}
library(astsa)
plot(soi)
```

Scatter plots show correlation in terms of lag.

To estimate autocorrelation, compute the correlation coefficient between the time series and itself at various lags.

Correlation at lag 1 (e.g. positive) and lag 6 (e.g. negative).  In this case 6 periods are months where correlation changes between summer and winter.

```{r}
par(mfrow=1:2, mar=c(4,4,2,1))
plot(lag(soi,-1), soi)
legend("topleft",legend=round(cor(soi[-1], soi[-length(soi)]),3))
plot(lag(soi,-6), soi)
legend("topleft", legend=round(cor(soi[-(1:6)], soi[-((length(soi)-5):length(soi))]),3))
```

# Random Walk Trend

The global temperature deviation is an example of Random Walk (not stationary) but its differenced data is stationary.

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

# Trend Stationary

Stationarity around a trend, differencing still works.

```{r}
plot(cbind(chicken, diff(chicken)), main="Monthly price of a pound of chicken")
```

# Nonstationarity in trend and variability

First log can stabilize the variance, then difference can detrend.

```{r}
plot(cbind(jj, log(jj), diff(log(jj))), main="Johnson and Johnson Quarterly Earnings Per Sharen")
```

Often time series are generated as $X_t=(1+p_t)X_{t−1}$ meaning that the value of the time series observed at time $t$ equals the value observed at time $t−1$ and a small percent change $pt$ at time $t$.

A simple deterministic example is putting money into a bank with a fixed interest $p$. In this case, $X_t$ is the value of the account at time period $t$ with an initial deposit of $X_0$.

Typically, $p_t$ is referred to as the return or growth rate of a time series, and this process is often stable.

For reasons that are outside the scope of this course, it can be shown that the growth rate $p_t$ can be approximated by
$Y_t=log(X_t)−log(X_{t−1})\approx p_t$.

# Stationary Time Series: ARMA

Why to use ARMA models for stationary time series data?

Wold Decomposition

Wold proved that any stationary time series may be represented as a linear combination of white noise:

$$X_t = W_t + a_1 W_{t-1} + a_2 W_{t-2} + ...$$
For constants $a_1, a_2, ...$

Any ARMA model has this form, which means they are suited to modeling time series.

Note: Special case of MA($q$) is already of this form, where constants are $0$ after $q$-th term.

# Generating ARMA using arima.sim()

    arima.sim(model, n, ...)

* model is a list with order of the model as c(p, d, q) and the coefficients 
    * p is the order of the AR
    * q is the order of the MA
* n is the length of the series

## MA(1)

Recall $$X_t = W_t + \theta W_{t-1}%$$
Given $\theta=0.9$ $$X_t = W_t + 0.9W_{t-1}$$
```{r}
plot(arima.sim(model=list(order=c(0,0,1), ma=0.9), n=100))
```

## AR(2)

Recall $$X_t = \phi X_{t-2} + W_t$$
Given $\phi=-0.9$ $$X_t =-0.9 X_{t-2} + W_t$$
```{r}
plot(arima.sim(model=list(order=c(2,0,0), ar=c(0,-0.9)), n=100))
```

Notice the data are somewhat cyclic.

Generating and plotting WN

```{r}
plot(arima.sim(model = list(order=c(0,0,0)), n=100))
```

# AR and MA Models

You can't identify a model simply looking at the data:

```{r}
x <- arima.sim(list(order = c(1, 0, 0), ar = -.7), n = 200)
y <- arima.sim(list(order = c(0, 0, 1), ma = -.7), n = 200)
par(mfrow = c(1, 2))
plot(x, main = "AR(1)")
plot(y, main = "MA(1)")
```

## ACF and PACF identify Model's orders

Autocorrelation Function and Partial Autocorrelation Function.

function | AR(p)          | MA(q)         | ARMA(p, q)
---------|----------------|---------------|-----------
ACF      | Tails off      | Cut off lag q | Tails off
PACF     | Cuts off lag p | Tails off     | Tails off

### AR(2) ACF/PACF
```{r}
par(mfrow=c(1,2))
invisible(acf2(arima.sim(model=list(order=c(2,0,0), ar=c(0,-0.9)), n=100)))
```

### MA(1)) ACF/PACF
```{r}
par(mfrow=c(1,2))
invisible(acf2(arima.sim(model=list(order=c(0,0,1), ma=0.9), n=100)))
```

# Estimation

* Estimation for time series is similar to using least squares for regression
    * For time series it's much harder and the results are not explicity.
* Estimates are obtained numerically using ideas of Gauss and Newton

## Estimation with astsa::sarima

### AR(2) with mean 50

$$(Today − Mean) = Slope \times (Y esterday − Mean) + Noise$$
$$X_t = 50 + 1.5(X_{t-1} - 50) - .75(X_{t-2} -50) + W_t$$
```{r}
x <- arima.sim(list(order = c(2,0,0), ar = c(1.5,-.75)), n=200) + 50
x_fit <- sarima(x, p=2, d=0, q=0, details=FALSE)
x_fit$ttable
```

As expected, ACF tails off and PACF cuts off lag 2: AR(2)
```{r}
invisible(acf2(x))
```

### MA(1) with mean 0

$$X_t = W_t + \theta W_{t-1}%$$
$$X_t = W_t + .7 W_{t-1}%$$
```{r}
y <- arima.sim(list(order = c(0,0,1), ma=-.7), n=200)
y_fit <- sarima(y, p=0, d=0, q=1, details=FALSE)
y_fit$ttable
```

As expected, ACF cuts off lag 1 and PACF tails off: MA(1)
```{r}
invisible(acf2(y))
```

# AR and MA together

Autocorrelation and autocorrelated errors

$$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$$
Given $\phi=.9$ and $\theta=.4$, $$X_t = .9 X_{t-1} + W_t + .4 W_{t-1}$$

```{r}
x <- arima.sim(list(order = c(1,0,1), ar=.9, ma=-.4), n=200)
plot(x, main = "ARMA(1, 1)")
```

For an ARMA model both ACF/PACF tail off:
```{r}
invisible(acf2(x))
```

Estimation from an ARMA(1,1) model with mean 50

$$X_t = .9 X_{t-1} + W_t + .4 W_{t-1} + 50$$

```{r}
x <- arima.sim(list(order = c(1, 0, 1), ar = .9, ma = -.4 ), n = 200) + 50
x_fit <- sarima(x, p = 1, d = 0, q = 1, details=FALSE)
x_fit$ttable
```

Estimation from a simulated ARMA(2,1)

$$X_t = X_{t-1} + -.9 X_{t-2} + W_t + .8 W_{t-1}$$
```{r}
x <- arima.sim(list(order = c(2, 0, 1), ar = c(1, -.9), ma = .8 ), n = 250)
plot(x)
```
```{r}
x_fit <- sarima(x, p = 2, d = 0, q = 1, details=FALSE)
x_fit$ttable
```

# Model choice and Residual Analysis

## AIC and BIC

It's often two ore more models seems reasonable. It's tipically a good idea to fit a few models before deciding on the best one. 

AIC and BIC ate the two most popular methods to choose the best model.

As more parameters are include in the model the error gets smaller whether or not the parameters are needed.

* AIC and BIC measure the error and penalize (differently) for adding parameters.  
* For example, AIC has $k=2$ and BIC has bigger penalty $k=log(n)$ and tends to choose a model with fewer parameters but the two however often agree.  
* Goal: find the model with the smallest AIC or BIC

## Model Choice: AR(1) vs. MA(2)

Quarterly U.S. GNP
```{r}
gnpgr <- diff(log(gnp))
plot(cbind(gnp, gnpgr), main="Quarterly U.S. GNP")
```

AR(1)
```{r}
sarima(gnpgr, p = 1, d = 0, q = 0, details=FALSE)[c("AIC","BIC")]
```
MA(2)
```{r}
sarima(gnpgr, p = 0, d = 0, q = 2, details=FALSE)[c("AIC","BIC")]
```

AIC prefers the MA(2) while BIC preferes the AR(1).

Since the AR(1) is a simpler model it'll be appropriate to prefer that.

# Residual Analsys

The basic of the residual analysis is the same as in the Regression.  We want to make sure that residuals are white gaussian noise, otherwise, we haven't found the best model.

## AR(1) fit to the Gross Rate of Quarterly U.S.GNP

1) The plot for standardized residuals shouldn't be expected for patterns. It's difficult to tell it's white noise right from the plot but it's easy to tell it's not. For example, are there obvious patterns in the residuals?

2) The ACF of the residuals can be used to assess whiteness.  99% of the values should be between dashed blue lines.

3) The Q-Q plot assess the normality. If the residuals are normal the points will lie over the line.  There are often extreme values at the ends.  As long as there aren't huge departures from the line then the normal assumption is reasonable.

4) There is Q-statistic that tests for whiteness of the residuals.  As long as the most points are above the blue dash lines then it can be safely assume the noise is white.  If however many of the points are bellow the line than there's still correlation left in the residuals and it should try another model or add a parameter.

Here the residuals are fine.

```{r fig.height=7, fig.width=7}
sarima(gnpgr, p = 1, d = 0, q = 0)
```

## Find the best model for Annual Varve log difference Series

```{r}
data(varve)
dl_varve <- diff(log(varve))
plot(cbind(varve, dl_varve), main="Annual Varve Series")
```

### Fitting an MA(1) model
```{r}
sarima(dl_varve,0,0,1, details=FALSE)[c("AIC", "BIC")]
```

### Fitting an MA(2) model improved the AIC and BIC over the MA(1) model
```{r}
sarima(dl_varve,0,0,2, details=FALSE)[c("AIC", "BIC")]
```

### Fitting an ARMA(1,1) model improved the AIC and BIC over the MA(2) model
```{r}
sarima(dl_varve,1,0,1, details=FALSE)[c("AIC", "BIC")]
```

AIC and BIC help you find the model with the smallest error using the least number of parameters. The idea is based on the parsimony principle, which is basic to all science and tells you to choose the simplest scientific explanation that fits the evidence.

## Residual Analysis of models MA(1) and ARMA(1,1) for the Annual Varve log difference Series

### MA(1) model
```{r fig.height=7, fig.width=7}
sarima(dl_varve,0,0,1)
```

**Residual analysis:**

1. The standardized residuals should behave as a white noise sequence with mean zero and variance one. Examine the residual plot for departures from this behavior.

    It's hard to say.  It seems not so white.

2. The sample ACF of the residuals should look like that of white noise. Examine the ACF for departures from this behavior.

    There are 3 values outside the range, possibly not white noise.

3. Normality is an essential assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers.

    There's reasonable normality with few outliers.

4. Use the Q-statistic plot to help test for departures from whiteness of the residuals.

    All p-values are bellow the limit, confirming it there's still correlation in the residuals and this is not a good model fit.

### ARMA(1,1) model
```{r fig.height=7, fig.width=7}
sarima(dl_varve,1,0,1)
```

**Residual Analysis:**

1. The standardized residuals should behave as a white noise sequence with mean zero and variance one. Examine the residual plot for departures from this behavior.

    As in the previous case, it's hard to say.  It seems not so white.

2. The sample ACF of the residuals should look like that of white noise. Examine the ACF for departures from this behavior.

    All values are inside the range, possibly white noise.

3. Normality is an essential assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers.

    As in the previous case, there's reasonable normality with few outliers.

4. Use the Q-statistic plot to help test for departures from whiteness of the residuals.

    All p-values are above the limit, confirming there's no correlation in the residuals and this is a good model fit.

## Fitting a model to Crude oil, WTI spot price FOB

### Calculate approximate oil returns
```{r}
data(oil)
oil_returns <- diff(log(oil))
```

### Plot oil_returns.

Notice that there are a couple of outliers prior to 2004. Convince yourself that the returns are stationary.
```{r}
plot(cbind(oil, oil_returns), main="Crude oil, WTI spot price FOB ($/barrel), weekly data, 2000 to mid-2010")
```

### Plot the P/ACF pair for oil_returns
```{r}
acf2(oil_returns)
```

### Fit a model to oil_returns

From the P/ACF pair, it is apparent that the correlations are small and the returns are nearly noise. But it could be that both the ACF and PACF are tailing off. If this is the case, then an ARMA(1,1) is suggested. Fit this model to the oil returns using sarima().

```{r fig.height=7, fig.width=7}
sarima(oil_returns, 1,0,1)
```

It seems not a good fit because most of the p-values are bellow the limit.

# ARIMA - Integrated ARMA

A time series is called ARIMA(p,d,q) if the differenced series (of order d) is ARMA(p,q).

## Identifying ARIMA

A time series exhibits ARIMA behavior if the differenced data has ARMA behavior.

Simulation ARIMA(p = 1, d = 1, q = 0)
```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
par(mfrow=1:2)
plot(x, main = "ARIMA(p = 1, d = 1, q = 0)")
plot(diff(x), main = "ARMA(p = 1, d = 0, q = 0)")
```

## ACF and PACF of an Integrated ARMA

ACF decays in a linear fashion and PCF is almost 1 at lag one.

```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
acf2(x)
```

## ACF and PACF of a differenced ARMA

Indicates and ARMA model for the differenced data: ACF tails off, PACF cutsoff at lag one.

```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
acf2(diff(x))
```

## Weekly Oil Prices

```{r}
par(mfrow=2:1)
plot(oil, main="Weekly Price of Oil: Random Walk")
plot(diff(oil), main="Differenced Price of Oil: Stationary")
invisible(acf2(diff(oil)))
```

Both ACF and PACF tail off, suggesting an ARMA(1,1) model on the differenced data or an ARIMA(1,1,1) on the data itself.

## ARIMA - Plug and Play

Simulated data from the integrated model $$Y_t=.9Y_t−1+W_t$$ where $Y_t=\nabla X_t=X_t−X_{t−1}$. In this case, the model is an ARIMA(1,1,0) because the differenced data are an autoregression of order one.

Plot x
```{r}
x <- arima.sim(model = list(order = c(1, 1, 0), ar = .9), n = 200)
plot(x)
```

Plot the P/ACF pair of x
```{r}
acf2(x)
```

Plot the differenced data
```{r}
plot(diff(x))
```

Plot the P/ACF pair of the differenced data

Note how they imply an AR(1) model for the differenced data.  ACF tails off and PACF cuts off.

```{r}
acf2(diff(x))
```

Differencing the data in your ARIMA(1,1,0) model makes it stationary and allows for further analysis

## Slightly more complicated simulated ARIMA

250 observations from the ARIMA(2,1,0) model with drift given by

$$Y_t=1+1.5Y_{t−1}−.75Y_{t−2}+W_t$$

where $Y_t=\nabla X_t=X_t−X_{t−1}$

```{r}
x <- arima.sim(model = list(order = c(2, 1, 0), ar = c(1.5, -.75)), n = 250, mean=1)
par(mfrow=2:1)
plot(x, main=expression(x[t]))
plot(diff(x), main=expression(paste(y[t], "=", x[t]-x[t-1])))
```

Plot sample P/ACF of differenced data and determine model
```{r}
invisible(acf2(diff(x)))
```

Estimate parameters and examine output
```{r}
sarima(x, 2,1,0, details=FALSE)$ttable
```

The estimated parameters are very close to 1.5 and -0.75

## Global Warming

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

Plot the sample P/ACF pair of the differenced data
```{r}
invisible(acf2(diff(globtemp)))
```

Either:

1. The ACF and the PACF are both tailing off, implying an ARIMA(1,1,1) model.

2. The ACF cuts off at lag 2, and the PACF is tailing off, implying an ARIMA(0,1,2) model.

3. The ACF is tailing off and the PACF cuts off at lag 3, implying an ARIMA(3,1,0) model. Although this model fits reasonably well, it is the worst of the three (you can check it) because it uses too many parameters for such small autocorrelations.

Fit an ARIMA(1,1,1) model to globtemp
```{r}
sarima(globtemp, 1,1,1, details = FALSE)[c("ttable", "AIC", "BIC")]
```
All parameters are significant.

Fit an ARIMA(0,1,2) model to globtemp
```{r}
sarima(globtemp, 0,1,2, details = FALSE)[c("ttable", "AIC", "BIC")]
```

All parameters are significant and both AIC and BIC shows this model is a better fit.

Fit an ARIMA(3,1,0) model to globtemp
```{r}
sarima(globtemp, 3,1,0, details = FALSE)[c("ttable", "AIC", "BIC")]
```

The drift estimate is not significant and AIC/BIC are worse than previous models.

# ARIMA Diagnostics

Diagnostics ARIMA models is the same as for ARMA models.

Once a model seems reasonable, try adding a parameter to see if it makes an improvement.  If it does so, change the model, otherwise the model is fitted.

## Weekly Oil Prices ARIMA(1,1,1)?

For this example, the Oil Series is subset until 2006.
```{r}
par(mfrow=2:1, mar=c(2,3,1,1))
data(oil)
plot(oil)
abline(v=2006, lty=2)
oil2006 <- window(oil, end = 2006)
plot(oil2006)
```

The P/ACF of the detrended oil price appears to tail off, suggesting an ARIMA(1,1,1)
```{r}
par(mfrow=2:1)
plot(oil2006, main="Weekly Price of Oil until 2006")
plot(diff(oil2006), main="Differenced Price of Oil until 2006")
invisible(acf2(diff(oil2006)))
```

```{r fig.height=7, fig.width=7}
sarima(oil2006, p = 1, d = 1, q = 1)["ttable"]
```

The parameter estimates $\phi$ and $\theta$ are all significant but the constant drift $\mu$.

The residual analysis looks fine too.

** Overfit: ARIMA(2, 1, 1) and ARIMA(1, 1, 2)**

We can try to add and AR parameter and an MA parameters to see it they make a difference.

```{r}
oil_fit1 <- sarima(oil2006, p = 2, d = 1, q = 1, details=FALSE)
oil_fit1$ttable
```

```{r}
oil_fit2 <- sarima(oil2006, p = 1, d = 1, q = 2, details=FALSE)
oil_fit2$ttable
```

We can see in each case the `ar2` and `ma2` parameters are not significant an the other former parameters are practically unchanged. Hence, the original model ARIMA(1,1,1) fits well.

## Diagnostics - Simulated Overfitting

One way to check an analysis is to overfit the model by adding an extra parameter to see if it makes a difference in the results. If adding parameters changes the results drastically, then you should rethink your model. If, however, the results do not change by much, you can be confident that your fit is correct.

Simulated 250 observations from an ARIMA(0,1,1) model with MA parameter .9
```{r}
x <- arima.sim(model=list(order=c(0,1,1), ma=.9), n=250)
plot(cbind(x, diff(x)), main=expression(paste("Simulated ARIMA(0,1,1) with ",theta," = .9")))
```

Plot sample P/ACF pair of the differenced data shows ACF cuts off lag 1 and PCF tails off, confirming an MA model.

```{r}
invisible(acf2(diff(x)))
```

Fit the first model, compare parameters, check diagnostics

```{r}
sarima(x, 0,1,1)
```

The ARIMA(0,1,1) easily identified with good parameter estimates.

Fit the second model adding a parameter: ARIMA(0,1,2)

```{r}
sarima(x,0,1,2)
```

As you can see from the t-table, the second MA parameter is not significantly different from zero and the first MA parameter is approximately the same in each run. Also, the AIC and BIC both increase when the parameter is added. In addition, the residual analysis of your ARIMA(0,1,1) model is fine. All of these facts together indicate that you have a successful model fit.

## Diagnostics - Global Temperatures

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

Fit ARIMA(0,1,2) to globtemp and check diagnostics

```{r}
sarima(globtemp, 0,1,2)
```

Fit ARIMA(1,1,1) to globtemp and check diagnostics

```{r}
sarima(globtemp, 1,1,2)
```

The model diagnostics suggest that both the ARIMA(0,1,2) and the ARIMA(1,1,1) are reasonable models. However, the AIC and BIC suggest that the ARIMA(0,1,2) performs slightly better, so this should be your preferred model.

You can use overfitting to assess the final model. For example, try fitting an ARIMA(1,1,2) or an ARIMA(0,1,3) to the data.  The both show much higher AIC and BIC.

```{r}
sarima(x, 0,1,3)
```

```{r}
sarima(x, 1,1,2)
```

# Forecasting ARIMA

## Forecasting ARIMA Processes

* The model describes how the dynamics of the time series behave over time  
* Forecasting simply continues the model dynamics into the future  
* Use sarima.for() to forecast in the astsa-package.  It's similar to samira() but specifies the forescasting horizon

Forecast 52 weeks from 2006 on shown in red.  The dark grey area denotes +/- 1 Root Mean Squared prediction Error (RMSE) bounds and the light grey ribbon denotes +/- 2 RMSE bounds which represents 95% prediction interval.
```{r}
data(oil)
oil2006 <- window(oil, end = 2006)
oil2007 <- window(oil, end = 2007)
invisible(sarima.for(oil2006, n.ahead = 52, 1, 1, 1))
lines(oil2007, lty=3)
```

## Forecasting Simulated ARIMA

Simulated 120 observations from an ARIMA(1,1,0) model with AR parameter .9.
```{r}
y <- arima.sim(model=list(order=c(1,1,0), ar=.9), n=120)
```
Taking the first 100 observations for model fitting.
```{r}
x <- window(y, end=100)
plot(cbind(x, diff(x)), main="Simulated ARIMA(1,1,0) AR=.9")
```

Plot P/ACF pair of differenced data show ACF tails off and PACF cuts off lag 1 confirming AR(1)
```{r}
invisible(acf2(diff(x)))
```

Fitting the AR(1,1,0) model and checking t-table and diagnostics confirm parameters estimate.
```{r}
sarima(x, 1,1,0)
```

Forecast the data 20 time periods ahead

```{r}
invisible(sarima.for(x, n.ahead = 20, p = 1, d = 1, q = 0) )
lines(y)
```

## Forecasting Global Temperatures

Forecasting the annual global temperature deviations globtemp to 2050.

```{r}
data(globtemp)
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations until 2015")
```

As checked before, ARIMA(0,1,2) is the best fit for this data.  It can be re-checked by fitting an ARIMA(0,1,2) to globtemp.
```{r}
sarima(globtemp, 0,1,2)
```

Forecasting data 35 years into the future
```{r}
sarima.for(globtemp, 35, 0,1,2)
```

# Seasonal ARIMA

## Pure Seasonal Models

* Often collect data with a known seasonal component
* The letter S denotes de seasonal period
* Monthly Airline Passenger Numbers 1949-1960 (1 cycle every S = 12 months)
* Johnson and Johnson Quarterly Earnings Per Share 1960-1980 (1 cycle every S = 4 quarters)

```{r fig.height=4, fig.width=9}
par(mfrow=1:2)
plot(AirPassengers)
plot(jj)
```

Although not realist it's instructive to first consider pure seasonal models such as an $SAR(P=1)_{S=12}$

$$X_t = \phi X_{t-12} + W_t$$
The model which wight be appropriate for average monthly temperatures states that what happens this month depends on what happened in the same month last year plus a little bit of noise.  January is tipically small while June-July are tipically large.

### ACF and PACF for Pure Seasonal Models

The behavior of the P/ACF for seasonal models is similar to non-seasonal models, but things happend only at the seasons 1S, 2S, 3S...

function | $SAR(P)_s$      | $SMA(Q)_s$     | $SARMA(P, Q)_s$
---------|-----------------|----------------|----------------
ACF*     | Tails off       | Cut off lag QS | Tails off
PACF*    | Cuts off lag PS | Tails off      | Tails off

* the values at non-seasonal lags are zero

### Fit a Pure Seasonal Model

Simulating 250 observations from a pure seasonal model given by $$X_t=.9X_{t−12}+W_t+.5W_{t−12}$$,

```{r include=FALSE}
x <- structure(c(-3.06314410851646, -1.99732239404243, -3.92496515955175, 
5.37037489799919, 7.47036620147213, 0.50232825519928, 2.47700476652652, 
-10.093009933726, -3.46214141941794, 1.8348984442248, 3.80154785075141, 
1.8529052310628, -1.94517423802062, -1.88141894129477, -4.78283162564179, 
4.36126202555202, 7.15892394792458, 2.69850985105832, 0.2368271663589, 
-9.93283704364601, -3.40560211070745, 0.718450701795834, 2.71303338489415, 
2.30937975417519, -1.3080481160039, -0.57269108567957, -5.37028065976083, 
3.05287811789994, 7.7490641112883, 3.92561265770144, -0.354420749430642, 
-10.3263873516376, -1.30215365170056, 1.79631778311515, 1.53693412237362, 
4.59585834508521, -0.938106679437896, -0.753267388669721, -5.05940525945444, 
3.34599675994212, 7.31869879297882, 2.80153052642243, 0.236487689404324, 
-9.54099616253736, -1.46635141163789, 3.82921215981755, 1.56204939544822, 
3.93420591868536, -0.7946319968161, -0.319685914563873, -4.60688948601893, 
2.9466130065086, 6.47853409424339, 0.402653885790053, 0.41278955180389, 
-8.06925618912638, -2.51177993469698, 4.10457260430788, 0.448812670504336, 
1.27389507646431, -0.56086719927172, -0.345700505510054, -2.93266019912447, 
2.5253986879829, 5.87612461475545, -1.37413176347416, -0.833081972176348, 
-8.19281706265864, -1.46459076734885, 5.50166256646988, 0.145363753418299, 
1.33649676150988, -0.0972124498810977, 0.892847718128307, -2.44650749829833, 
2.86850932399444, 4.52193529885673, -1.13335736448006, -0.960570990724114, 
-8.42967948685017, -1.32447969107472, 6.8564711156652, 0.560600335605541, 
1.84207587432158, -0.453546827013603, 2.7858159247379, -4.90832316591259, 
2.90947878195027, 3.65013088862491, -0.681301606952241, -1.06422629850917, 
-6.47488999712745, 0.313153620252474, 6.8491380864795, 2.60478602108151, 
3.12897714034944, -0.62655683511164, 2.90431093742726, -6.02331361064574, 
1.97552385138561, 3.74475417419421, -1.20737868743937, -0.231460642833374, 
-5.56874792845922, 0.116360540492759, 4.87372950172773, 3.7485218535884, 
4.21619028768921, -0.801217994638757, 2.66898320772968, -3.86649211891936, 
3.5258856530634, 3.60995410505584, -0.29816579413972, -0.36551621840757, 
-5.14806193402198, -1.4648962308066, 2.25923587125507, 3.21431936834948, 
4.78864290850489, -0.783745434205344, 2.85817993780096, -3.76443693750173, 
3.88465658771262, 2.72543098759966, 1.29694133615814, -1.53425673864416, 
-4.08095656516748, -2.08063716648024, -0.0502145320031677, 1.17996777385966, 
4.58161185650757, -2.74152364599098, 1.9898706471021, -2.8282569061318, 
4.16920100454489, 0.753004492475234, 2.18951940654714, -1.83834565496867, 
-2.8211650107456, -4.0667327208016, -1.37969624710891, 0.983259622533417, 
4.56122041377081, -3.01142452099303, 0.568549520640781, -3.25466215343091, 
2.01180399426369, -0.396281095357573, 1.63027968797037, -1.76596306315486, 
-2.18743465790239, -2.50699909712629, -1.29555048623182, 1.74474031677116, 
4.974877345529, -3.10174912874083, 1.35999794900987, -2.61130286858684, 
-0.109013347162008, 1.38760722798552, 1.72710916541064, -2.49007019610945, 
-3.81347687393752, -1.95747428945591, -0.571548843677568, 2.37916338912546, 
5.92002612160869, -5.05410923572105, 1.69803265578682, -2.62143872558672, 
-1.53876681010285, 1.8015665668533, 1.93171559919034, -1.40558001405695, 
-5.83866265017111, -3.01082036040923, -0.790187870863721, 2.08041299781313, 
4.14373554127184, -6.0719141451156, 2.37434930252529, -2.65873047160315, 
-2.09769899773427, 0.72198304541978, 2.4430439777415, -1.12219463148449, 
-5.97950519980165, -4.84978836790319, -0.712450133314355, 1.86830130297215, 
2.12744280212214, -6.85442291392057, 1.9099449970341, -3.20517896639948, 
-1.13947531511847, 0.580967282972911, 1.52650336735008, -2.05082449787386, 
-6.72410092553833, -4.61244098112566, -1.23560536296262, 0.589635815821389, 
0.82803132709459, -7.43420378123948, 0.602117269663443, -4.28772787509716, 
-1.82473744814939, -0.24184375070731, 0.107012432262849, -2.5409305452456, 
-7.61825131011511, -4.06603167809667, 0.323159779284364, 0.166599532011676, 
0.144974103254228, -6.40423902661882, 0.584768320408626, -3.07473338291192, 
-3.8120521189765, -2.48378311277287, 0.783067387172396, -2.51192594003257, 
-7.77006332225859, -4.38928360660089, 2.42606742378432, 0.607070361914789, 
0.47021646717987, -5.93375792343004, 1.55061160815523, -1.28834139979336, 
-3.31239369475639, -3.32106880504781, 2.47840155720084, -1.35132988684253, 
-10.6928953454591, -5.3752803784184, 3.16127520668831, -0.473723581058986, 
2.11026823386754, -6.45289829864847, 0.998550198635667, -0.472518186816309, 
-2.44178032903136, -3.73976632602486, 3.27058601265919, -2.57028328651033, 
-10.6444086692551, -3.97192873686762, 2.40821723736333, 0.067797058355085, 
3.37461194097261), .Tsp = c(1, 21.9166666666667, 12), class = "ts")
```

which we would denote as a SARMA(P = 1, Q = 1)$_{S = 12}$.

Three years of data and the model ACF and PACF are plotted.
```{r}
Months <- c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D")
plot(window(x, start=c(1,1), end=c(4,1)), type="l", col="gray", xaxt="n", xlab="year", main="Seasonal ARMA(1,1)")
abline(v=1:4, lty=2, col="gray")
points(window(x, start=c(1,1), end=c(4,1)), pch=Months, col=1:4)
axis(1, 1:4)
```

You will compare the sample ACF and PACF values from the generated data to the true values displayed to the right. Plot sample P/ACF to lag 60 and compare to the true values
```{r}
invisible(acf2(x, max.lag = 60))
```

Fit the seasonal model to x

```{r}
sarima(x, p = 0, d = 0, q = 0, P = 1, D = 0, Q = 1, S = 12)
```

Fitting a seasonal model using sarima() requires a few additional arguments, but follows the same basic process as nonseasonal models.

## Mixed Seasonal Models

Purely seasonal time series are rare and tipically it's needed to mix the seasonal part with the non-seasonal part

* Mixed model: SARIMA(p, d, q) x (P, D, Q)$_S$ model
    * lower case letters refer to the non-seasonal components
    * capital letters refere to the seasonal component

* Consider a SARIMA(0, 0, 1) x (1, 0, 0)$_12$ model

$$X_t = \phi X_{t-12} + W_t + \theta W_{t-1}$$

* SAR(1): This month's value $X_t$ is related to last year’s value $X_{t-12}$.

* MA(1): This month’s value related to last month’s shock or error $W_{t-1}$

### ACF and PACF of SARIMA(0,0,1) x (1,0,0)$_{S=12}$

* The ACF and PACF for this mixed model with the Seasonal AR parameter of .8 and the Non-seasonal MA parater of -0.5

$$X_t = .8X_{t-12} + W_t - .5W_{t-1}$$

```{r}
#TODO: plot
```

### Air Passengers

Monthly totals of international airline passengers, 1949-1960

```{r fig.height=7, fig.width=7}
x <- AirPassengers
plot.ts(cbind(x, log(x), diff(log(x)), diff(diff(log(x)),12)), main=NA, yax.flip = T)
```

* Notice that the x variance is increasing so logging the data is appropriate  
* Notice that after the variance is stabilized there's an obvious trend so differencing the data is needed  
* Looking at the differenced data notice there's a seasonal persistence of three cycles per year which can be removed by a seasonal difference  
* At the end the data look stationary and the difference data can be modeled  
* At this point it has d = 1, D = 1 with S = 12

### Air Passengers: ACF and PACF of ddlx

```{r}
invisible(acf2(diff(diff(log(x)),12)))
```

* Seasonal:
    * ACF cutting off at lag 1S (S = 12)
    * PACF tailing off at lags 1S, 2S, 3S,,
    & They suggest a Seasonal SMA(1) so capital P = 0 and capital Q = 1

* Non-Seasonal: ACF and PACF both tailing off which sugests an ARMA(1,1) so little p = 1 and and little q = 1

Finally we concluded that $$SARIMA(1, 1, 1) \times (0, 1, 1)_{12}$$ should be appropriate on the log data.

Fitting $SARIMA(1, 1, 1) \times (0, 1, 1)_{12}$ didn't find a significant estimate for slope $\phi$ `ar1`
```{r}
airpass_fit1 <- sarima(log(AirPassengers),
                       p = 1, d = 1, q = 1,
                       P = 0, D = 1, Q = 1,
                       S = 12, details = FALSE)
airpass_fit1$ttable
```

Removing a parameter by fitting $SARIMA(0, 1, 1) \times (0, 1, 1)_{12}$ found all significant estimates.

```{r}
airpass_fit2 <- sarima(log(AirPassengers),
                       p = 0, d = 1, q = 1,
                       P = 0, D = 1, Q = 1,
                       S = 12)
airpass_fit2$ttable 
```

In the residual diagnostics shown for this model everything looks fine and it's a fine good model.

### Fit a Mixed Seasonal Model

Pure seasonal dependence such as that explored earlier in this chapter is relatively rare. Most seasonal time series have mixed dependence, meaning only some of the variation is explained by seasonal trends.

Recall that the full seasonal model is denoted by SARIMA(p,d,q)x(P,D,Q)$_S$ where capital letters denote the seasonal orders.

As before, this exercise asks you to compare the sample P/ACF pair to the true values for some simulated seasonal data and fit a model to the data using sarima(). This time, the simulated data come from a mixed seasonal model, SARIMA(0,0,1)x(0,0,1)12. The plots on the right show three years of data, as well as the model ACF and PACF. Notice that, as opposed to the pure seasonal model, there are correlations at the nonseasonal lags as well as the seasonal lags.

As always, the astsa package is preloaded. The generated data are in x.

```{r include=FALSE}
x <- structure(c(-1.24258374720122, -0.680065405764787, 1.356227518177, 
0.843129751454754, -0.408806843365211, 1.06190105901572, -1.08024262557284, 
3.00159491546997, 0.812483691538154, -0.388439348188513, -1.78832252504976, 
2.32060891463776, -3.26414890202722, 0.865968158278709, -0.0040275603529803, 
0.288523441820578, 0.855135845954578, 1.44479256992233, -1.08484353181202, 
2.425670844264, -2.20067533440617, -1.01353202763671, 0.127242155052631, 
1.32606121281615, -2.95797970756062, 2.63475995582429, -1.20874438429627, 
0.287662257141432, 0.0245859091941785, 2.2245055031213, -0.792188814850646, 
2.5796602063052, -2.43990935356691, -1.96127805253682, 1.73227117119116, 
-0.62003498069671, -1.06261298992055, 1.14760117568205, -0.552948084538006, 
1.19190441113015, -1.64227367024056, 0.836119426659533, 1.0216916369614, 
0.844007095490518, 0.407152216390859, -1.23929379415373, -0.0928417038216168, 
-0.917543579660732, -0.543490765691083, 0.0172247711859918, 0.217795232257818, 
1.8945450491054, -1.62825206350099, 1.09160344389953, 1.42457219648912, 
-0.961606898017028, -1.40706883734458, 0.580322363778704, 0.128185491866183, 
-0.508999458166225, -0.379879950934173, 0.886123515219509, -1.13451492116038, 
2.31850146994214, -1.19853022476933, 2.70026830463454, 0.339923770476176, 
-1.39328379322504, -1.55349501794299, 1.14939004255453, 1.94961294374811, 
-0.562523389410428, -1.74610712398252, 2.43968382412471, -1.44885536602227, 
0.305652863330674, 0.494726388404694, 2.16951263135258, 1.0352446861911, 
0.186390831199457, 0.0435068193321523, 0.97226639671766, -1.72417560015489, 
1.31448854096215, -1.91227711003925, 1.81001644784018, 1.11064735274493, 
-1.51745153432704, 2.9504478583582, -1.68190718178578, 2.42246114996808, 
-1.52551391327789, 0.372090482921557, -0.502512460405299, -0.160197795117164, 
-1.41996772544633, -0.82620004077948, 1.20131888292962, 1.76421421529905, 
-1.75853694335753, 3.39174196133972, -0.872913529147085, 1.48877662399141, 
-2.76751501632078, 0.442146346606621, 0.170634395787808, -1.11746117938133, 
-0.756557807966695, 0.755501008378061, 0.930669498030656, -0.832317167753011, 
1.02845407278007, 1.17597268463627, -0.270305815576009, 0.818449456380573, 
-2.09643845378437, -0.233723822414456, 0.309935824632352, -1.01777410394759, 
2.88317133770344, -1.11912734825347, 0.201400789219922, -0.495380460964211, 
1.50637205475064, -0.695820638775496, 0.0209185325907438, 0.460910649185173, 
-2.81688418000931, 0.665074177042645, -0.769912579202173, 2.28344070001228, 
0.635142547821299, -2.87553109985384, -0.200819557812719, 1.10949558509179, 
0.666019999257468, 0.0957619319845518, -0.77559156239279, -2.02189263886315, 
2.1014210781206, -0.861213151561466, -1.65900864440935, 3.32411334183089, 
-0.427697232645185, 0.00246636526982758, -0.0628927718385216, 
0.0809976529599866, -0.0341324701288019, -1.02214194499063, 0.247304755707135, 
-2.83239516949754, 4.96695735439066, -2.34840086328062, -1.96340839992396, 
2.96605773063462, 0.316718658570475, 0.678206022202642, -1.14550434525026, 
-0.278673060617412, 1.63216832023144, -3.30816542882771, 1.18274563647405, 
0.875227995468977, 1.94119560761204, -1.42670032308748, -1.03594702614171, 
1.1954940440027, 1.42539786017312, 1.12642617037995, -3.35375037027888, 
1.02461994438544, 0.975775805412616, -1.00961249294996, -1.43692299805807, 
2.34904799446936, -0.451525449342903, 0.269027842092754, -0.244761325405202, 
-1.10742140413175, 2.44230226186268, -0.544083120740584, -0.113636639226151, 
-0.120831289535971, 1.01687227878275, -1.10730204555026, -0.678508794331513, 
0.356277373911747, -0.535434343682098, 0.584228564440411, 1.07467809949096, 
-1.72985781962077, 1.32108480777427, -1.50289691898113, 0.796840905154969, 
-0.713014482602043, 1.5986214554117, -1.55122284511062, 1.46220462343757, 
-1.5656779342176, -2.09407515444174, 1.15896830348926, 1.52026800389504, 
0.527674212921186, -0.479517750010528, 0.0196936885973087, -0.357459214203162, 
1.08823580189231, -0.935501818845785, 2.70710789152998, -0.052828272223102, 
-1.87636072334541, -1.16239690873568, 2.719218473836, -0.818013259486425, 
-0.350721964181141, 0.458936474803007, 0.650056218215348, -0.735326597551863, 
2.80497602006768, -1.15305675283549, 2.1709400539799, -0.00746223499060594, 
-0.53982776325546, -1.18611904819888, 1.69393876449315, 0.491210987581325, 
-3.2698508124788, 1.60513799074771, -0.256295840057811, 0.235412930690358, 
2.33425160659401, 1.16402525611574, -2.02378849646913, -0.174277639615686, 
1.58754699982943, -3.07940045107966, -1.28558894237328, 2.6796497040746, 
-2.624702875971, 0.280032128374774, -0.910473992517226, 0.789275686647129, 
1.67719546538224, 1.2913998066395, -2.9351850550229, 0.586979346495187, 
0.782896494798968, -0.7485494530492, -0.455095008124042, 1.18126973488548, 
-0.221305314592858, -1.71276313996773), .Tsp = c(1, 21.9166666666667, 
12), class = "ts")
```

```{r}
Months <- c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D")
plot(window(x, start=c(1,1), end=c(4,1)), type="l", col="gray", xaxt="n", xlab="year", main="Seasonal ARMA(1,1)")
abline(v=1:4, lty=2, col="gray")
points(window(x, start=c(1,1), end=c(4,1)), pch=Months, col=1:4)
axis(1, 1:4)
```

```{r}
invisible(acf2(x,max.lag = 60))
```

Fit the seasonal model to x

```{r}
sarima(x, p = 0, d = 0, q = 1, P = 0, D = 0, Q = 1, S = 12)
```

Time series data collected on a seasonal basis typcially have mixed dependence. For example, what happens in June is often related to what happend in May as well as what happened in June of last year.

### Data Analysis - Unemployment I

Plot the monthly US unemployment (unemp) time series from astsa. Note trend and seasonality.
```{r}
plot(unemp, main="Monthly U.S. Unemployment series (1948-1978, n = 372)")
```

Detrend and plot the data. Save this as d_unemp. Notice the seasonal persistence.  The log was not applied because data doesn't show heteroskedasticity.

```{r}
d_unemp <- diff(unemp)
plot(d_unemp)
```

Seasonally difference the detrended series and save this as dd_unemp. Plot this new data and notice that it looks stationary now.

```{r}
dd_unemp <- diff(d_unemp, lag = 12)  
plot(dd_unemp)
```

### Data Analysis - Unemployment II

Difference the data fully (as in the previous exercise) and plot the sample ACF and PACF of the transformed data to lag 60 months (5 years).

```{r}
dd_unemp <- diff(diff(unemp), lag = 12)
invisible(acf2(dd_unemp, max.lag = 60))
```

Consider that, for:

* the nonseasonal component: the PACF cuts off at lag 2 and the ACF tails off.

    AR(1): p=2, d=1, q=0

* the seasonal component: the ACF cuts off at lag 12 and the PACF tails off at lags 12,24,36...

    SMA(1)12: P=0, D=1, Q=1

Suggest and fit a model using sarima(). Check the residuals to ensure appropriate model fit.

```{r}
sarima(unemp, p = 2, d = 1, q = 0, P = 0, D = 1, Q = 1, S = 12)
```

### Data Analysis - Commodity Prices: Chicken

```{r fig.height=7, fig.width=15}
Months <- c("A", "S", "O", "N", "D", "J", "F", "M", "A", "M", "J", "J")
plot(chicken, col="dark grey", main="Monthly price of a pound of chicken from August 2001 to July 2016")
points(chicken, pch=Months, col=1:4, cex=.8)
abline(v=seq(from=2001, to=2016), lty=2, col="gray")
```

Removing trend and noticing some mixed seasonal behaviour
```{r fig.height=7, fig.width=15}
plot(diff(chicken), col="dark grey")
points(diff(chicken), pch=Months, col=1:4)
abline(v=seq(from=2001, to=2016), lty=2, col="gray")
```

Plot the sample ACF and PACF of the differenced data to lag 60 (5 years). After removing the trend, the sample ACF and PACF suggest an AR(2) model because the PACF cuts off after lag 2 and the ACF tails off. However, the ACF has a small seasonal component remaining. This can be taken care of by fitting an addition SAR(1) component.

```{r}
invisible(acf2(diff(chicken), max.lag=60))
```

Fit ARIMA(2,1,0) to chicken - not so good
```{r}
sarima(chicken, 2,1,0)
```

Fit SARIMA(2,1,0,1,0,0,12) to chicken - that works
```{r}
sarima(chicken, 2,1,0,1,0,0,12)
```

### Data Analysis - Birth Rate

```{r fig.height=7, fig.width=15}
Months <- c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D")
plot(birth, col="dark grey", main="U.S. Monthly Live Births (adjusted) in thousands, 1948-1979")
points(birth, pch=Months, col=1:4, cex=.8)
abline(v=seq(from=1948, to=1979), lty=2, col="gray")
```

```{r fig.height=7, fig.width=15}
Months <- c("F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D", "J")
plot(diff(birth), col="dark grey", main="U.S. Monthly Live Births (adjusted) in thousands, 1948-1979")
points(diff(birth), pch=Months, col=1:4, cex=.8)
abline(v=seq(from=1948, to=1979), lty=2, col="gray")
```

Plot the sample ACF and PACF of the differenced data to lag 60 (5 years). Notice the seasonal persistence at ACF.  This requires to take the 12-month seasonal difference from the differenced data.

```{r}
invisible(acf2(diff(birth), max.lag=60))
```

Taking the 12-month seasonal difference from the differenced data
```{r fig.height=7, fig.width=15}
Months <- c("F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D", "J")
plot(diff(diff(birth), 12), col="dark grey", main="U.S. Monthly Live Births (adjusted) in thousands, 1948-1979")
points(diff(diff(birth), 12), pch=Months, col=1:4, cex=.8)
abline(v=seq(from=1948, to=1979), lty=2, col="gray")
```

Plot P/ACF to lag 60 of seasonal differenced data.

```{r}
invisible(acf2(diff(diff(birth),12), max.lag=60))
```

* Seasonal component: SMA(1)
    * ACF cuts off at lag Q=1
    * PACF tails off

* Non-Seasonal component: MA(1)
    * ACF cuts off at lag q=1
    * PACF tails off

Conclude that an SARIMA(0,1,1)x(0,1,1)12 model seems reasonable.

Fitting SARIMA(0,1,1)x(0,1,1)$_{12}$ shows that all p-values are bellow the limit, confirming it there's still correlation in the residuals so the are not white noise and this is not a good model fit.

```{r}
sarima(birth, 0,1,1, 0,1,1, 12)
```

Adding and AR(1) term fitted a good model
```{r}
sarima(birth, 1,1,1, 0,1,1, 12)
```

The residual analysis from the first fit indicated that the residuals were not white noise. Hence, it was necessary to include an additional nonseasonal AR term to account for the extra correlation.

## Forecasting Seasonal ARIMA

Forecasting ARIMA Processes

* Once model is chosen, forecasting is easy because the model describes how the dynamics of the time series behave over time  
* Simply continue the model dynamics into the future  
* In the astsa package, use sarima.for() for forecasting

### Forecasting Air Passengers

* In the previous video, we decided that a SARIMA(0,1,1)x(0,1,1)12 model was appropriate

The grayt swatches represent the 1 and 2 Root Mean Squared prediction intervals.

```{r}
invisible(sarima.for(log(AirPassengers), n.ahead = 24, 0, 1, 1, 0, 1, 1, 12))
```

### Forecasting Monthly Unemployment

Previously, you fit an SARIMA(2,1,0, 0,1,1)12 model to the monthly US unemployment time series unemp. You will now use that model to forecast the data 3 years.

Fit your previous model to unemp and check the diagnostics.
```{r}
sarima(unemp, 2,1,0, 0,1,1, 12)
```

Forecast the data 3 years into the future
```{r}
sarima.for(unemp, n.ahead = 36, 2,1,0,0,1,1,12)
```

The forecast is able to replicate much of the seasonal variation in the original unemployment data.

### How Hard is it to Forecast Commodity Prices?

As previously mentioned, making money in commodities is not easy. To see a difficulty in predicting a commodity, you will forecast the price of chicken to five years in the future. When you complete your forecasts, you will note that even just a few years out, the acceptable range of prices is very large. This is because commodities are subject to many sources of variation.

Recall that you previously fit an SARIMA(2,1,0, 1,0,0)12 model to the monthly US chicken price series chicken. You will use this model to calculate your forecasts.

Fit the chicken model again and check diagnostics
```{r}
sarima(chicken, 2,1,0, 1,0,0, 12)
```

Forecast the chicken data 5 years into the future
```{r}
sarima.for(chicken, n.ahead=60, 2,1,0, 1,0,0, 12)
```

# What you’ve learned

* How to identify an ARMA model from data looking at ACF and PACF  
* How to use integrated ARMA (ARIMA) models for nonstationary time series  
* How to cope with seasonality