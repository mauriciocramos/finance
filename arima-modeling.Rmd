---
title: "ARIMA modeling"
author: "Maurício Collaça"
date: "11 de agosto de 2018"
output: 
  html_document: 
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```

# Time Series Regression Models

Regression: $Y_i = \beta X_i + \epsilon_i$ where $\epsilon_i$ is **white noise**

## White Noise (WN)

* independent normals with common variance  
* is basic building block of time series

## AutoRegression (AR)

$X_t = \phi X_{t-1} + \epsilon_t$ ($\epsilon_t$ is **white noise**)

Tipically time series data are correlated and assuming error are not correlated may lead to a bad forecast.  One way to overcome this problems is to use a Moving Average for the errors.

## Moving Average (MA)

$\epsilon_t = W_t + \theta W_{t-1}%$ ($W_t$ is **white noise**)

Errors $\epsilon_t$ at time t are correlated to the errors $\epsilon_{t-1}$ at time t-1 because the both have a "$W_{t-1}$"

## Autoregressive + Moving Average (ARMA)

$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$

Putting these two together is a model with autocorrelation and autocorrelated errors.

# Stationarity and Nonstationarity

In the context of time series, it is stationary when it is “stable”, meaning:

* the mean is constant over time (no trend)  
* the correlation structure remains constant over time

Stationarity means we can use simple averaging to estimate $\bar X$ correlation.

Pairs can be used to estimate correlation on different lags: (x1,x2)(x2,x3)... for lag 1.

This works because relation between contiguous values of the series remains the same overtime.

Similarly (x1,x3)(x2,x4)... for lag 2.

Stationary example: Southern Oscillation Index

Reasonable to assume stationary, but perhaps some slight trend.

```{r}
library(astsa)
plot(soi)
```

Scatter plots show correlation in terms of lag.

To estimate autocorrelation, compute the correlation coefficient between the time series and itself at various lags.

Correlation at lag 1 (e.g. positive) and lag 6 (e.g. negative).  In this case 6 periods are months where correlation changes between summer and winter.

```{r}
par(mfrow=1:2, mar=c(4,4,2,1))
plot(lag(soi,-1), soi)
legend("topleft",legend=round(cor(soi[-1], soi[-length(soi)]),3))
plot(lag(soi,-6), soi)
legend("topleft", legend=round(cor(soi[-(1:6)], soi[-((length(soi)-5):length(soi))]),3))
```

# Random Walk Trend

The global temperature deviation is an example of Random Walk (not stationary) but its differenced data is stationary.

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

# Trend Stationary

Stationarity around a trend, differencing still works.

```{r}
plot(cbind(chicken, diff(chicken)), main="Monthly price of a pound of chicken")
```

# Nonstationarity in trend and variability

First log can stabilize the variance, then difference can detrend.

```{r}
plot(cbind(jj, log(jj), diff(log(jj))), main="Johnson and Johnson Quarterly Earnings Per Sharen")
```

Often time series are generated as $X_t=(1+p_t)X_{t−1}$ meaning that the value of the time series observed at time $t$ equals the value observed at time $t−1$ and a small percent change $pt$ at time $t$.

A simple deterministic example is putting money into a bank with a fixed interest $p$. In this case, $X_t$ is the value of the account at time period $t$ with an initial deposit of $X_0$.

Typically, $p_t$ is referred to as the return or growth rate of a time series, and this process is often stable.

For reasons that are outside the scope of this course, it can be shown that the growth rate $p_t$ can be approximated by
$Y_t=log(X_t)−log(X_{t−1})\approx p_t$.

# Stationary Time Series: ARMA

Why to use ARMA models for stationary time series data?

Wold Decomposition

Wold proved that any stationary time series may be represented as a linear combination of white noise:

$$X_t = W_t + a_1 W_{t-1} + a_2 W_{t-2} + ...$$
For constants $a_1, a_2, ...$

Any ARMA model has this form, which means they are suited to modeling time series.

Note: Special case of MA($q$) is already of this form, where constants are $0$ after $q$-th term.

# Generating ARMA using arima.sim()

    arima.sim(model, n, ...)

* model is a list with order of the model as c(p, d, q) and the coefficients 
    * p is the order of the AR
    * q is the order of the MA
* n is the length of the series

## MA(1)

Recall $$X_t = W_t + \theta W_{t-1}%$$
Given $\theta=0.9$ $$X_t = W_t + 0.9W_{t-1}$$
```{r}
plot(arima.sim(model=list(order=c(0,0,1), ma=0.9), n=100))
```

## AR(2)

Recall $$X_t = \phi X_{t-2} + W_t$$
Given $\phi=-0.9$ $$X_t =-0.9 X_{t-2} + W_t$$
```{r}
plot(arima.sim(model=list(order=c(2,0,0), ar=c(0,-0.9)), n=100))
```

Notice the data are somewhat cyclic.

Generating and plotting WN

```{r}
plot(arima.sim(model = list(order=c(0,0,0)), n=100))
```

# AR and MA Models

You can't identify a model simply looking at the data:

```{r}
x <- arima.sim(list(order = c(1, 0, 0), ar = -.7), n = 200)
y <- arima.sim(list(order = c(0, 0, 1), ma = -.7), n = 200)
par(mfrow = c(1, 2))
plot(x, main = "AR(1)")
plot(y, main = "MA(1)")
```

## ACF and PACF identify Model's orders

Autocorrelation Function and Partial Autocorrelation Function.

function | AR(p)          | MA(q)         | ARMA(p, q)
---------|----------------|---------------|-----------
ACF      | Tails off      | Cut off lag q | Tails off
PACF     | Cuts off lag p | Tails off     | Tails off

### AR(2) ACF/PACF
```{r}
par(mfrow=c(1,2))
invisible(acf2(arima.sim(model=list(order=c(2,0,0), ar=c(0,-0.9)), n=100)))
```

### MA(1)) ACF/PACF
```{r}
par(mfrow=c(1,2))
invisible(acf2(arima.sim(model=list(order=c(0,0,1), ma=0.9), n=100)))
```

# Estimation

* Estimation for time series is similar to using least squares for regression
    * For time series it's much harder and the results are not explicity.
* Estimates are obtained numerically using ideas of Gauss and Newton

## Estimation with astsa::sarima

### AR(2) with mean 50

$$(Today − Mean) = Slope \times (Y esterday − Mean) + Noise$$
$$X_t = 50 + 1.5(X_{t-1} - 50) - .75(X_{t-2} -50) + W_t$$
```{r}
x <- arima.sim(list(order = c(2,0,0), ar = c(1.5,-.75)), n=200) + 50
x_fit <- sarima(x, p=2, d=0, q=0, details=FALSE)
x_fit$ttable
```

As expected, ACF tails off and PACF cuts off lag 2: AR(2)
```{r}
invisible(acf2(x))
```

### MA(1) with mean 0

$$X_t = W_t + \theta W_{t-1}%$$
$$X_t = W_t + .7 W_{t-1}%$$
```{r}
y <- arima.sim(list(order = c(0,0,1), ma=-.7), n=200)
y_fit <- sarima(y, p=0, d=0, q=1, details=FALSE)
y_fit$ttable
```

As expected, ACF cuts off lag 1 and PACF tails off: MA(1)
```{r}
invisible(acf2(y))
```

# AR and MA together

Autocorrelation and autocorrelated errors

$$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$$
Given $\phi=.9$ and $\theta=.4$, $$X_t = .9 X_{t-1} + W_t + .4 W_{t-1}$$

```{r}
x <- arima.sim(list(order = c(1,0,1), ar=.9, ma=-.4), n=200)
plot(x, main = "ARMA(1, 1)")
```

For an ARMA model both ACF/PACF tail off:
```{r}
invisible(acf2(x))
```

Estimation from an ARMA(1,1) model with mean 50

$$X_t = .9 X_{t-1} + W_t + .4 W_{t-1} + 50$$

```{r}
x <- arima.sim(list(order = c(1, 0, 1), ar = .9, ma = -.4 ), n = 200) + 50
x_fit <- sarima(x, p = 1, d = 0, q = 1, details=FALSE)
x_fit$ttable
```

Estimation from a simulated ARMA(2,1)

$$X_t = X_{t-1} + -.9 X_{t-2} + W_t + .8 W_{t-1}$$
```{r}
x <- arima.sim(list(order = c(2, 0, 1), ar = c(1, -.9), ma = .8 ), n = 250)
plot(x)
```
```{r}
x_fit <- sarima(x, p = 2, d = 0, q = 1, details=FALSE)
x_fit$ttable
```

# Model choice and Residual Analysis

## AIC and BIC

It's often two ore more models seems reasonable. It's tipically a good idea to fit a few models before deciding on the best one. 

AIC and BIC ate the two most popular methods to choose the best model.

As more parameters are include in the model the error gets smaller whether or not the parameters are needed.

* AIC and BIC measure the error and penalize (differently) for adding parameters.  
* For example, AIC has $k=2$ and BIC has bigger penalty $k=log(n)$ and tends to choose a model with fewer parameters but the two however often agree.  
* Goal: find the model with the smallest AIC or BIC

## Model Choice: AR(1) vs. MA(2)

Quarterly U.S. GNP
```{r}
gnpgr <- diff(log(gnp))
plot(cbind(gnp, gnpgr), main="Quarterly U.S. GNP")
```

AR(1)
```{r}
sarima(gnpgr, p = 1, d = 0, q = 0, details=FALSE)[c("AIC","BIC")]
```
MA(2)
```{r}
sarima(gnpgr, p = 0, d = 0, q = 2, details=FALSE)[c("AIC","BIC")]
```

AIC prefers the MA(2) while BIC preferes the AR(1).

Since the AR(1) is a simpler model it'll be appropriate to prefer that.

# Residual Analsys

The basic of the residual analysis is the same as in the Regression.  We want to make sure that residuals are white gaussian noise, otherwise, we haven't found the best model.

## AR(1) fit to the Gross Rate of Quarterly U.S.GNP

1) The plot for standardized residuals shouldn't be expected for patterns. It's difficult to tell it's white noise right from the plot but it's easy to tell it's not. For example, are there obvious patterns in the residuals?

2) The ACF of the residuals can be used to assess whiteness.  99% of the values should be between dashed blue lines.

3) The Q-Q plot assess the normality. If the residuals are normal the points will lie over the line.  There are often extreme values at the ends.  As long as there aren't huge departures from the line then the normal assumption is reasonable.

4) There is Q-statistic that tests for whiteness of the residuals.  As long as the most points are above the blue dash lines then it can be safely assume the noise is white.  If however many of the points are bellow the line than there's still correlation left in the residuals and it should try another model or add a parameter.

Here the residuals are fine.

```{r fig.height=7, fig.width=7}
sarima(gnpgr, p = 1, d = 0, q = 0)
```

## Find the best model for Annual Varve log difference Series

```{r}
data(varve)
dl_varve <- diff(log(varve))
plot(cbind(varve, dl_varve), main="Annual Varve Series")
```

### Fitting an MA(1) model
```{r}
sarima(dl_varve,0,0,1, details=FALSE)[c("AIC", "BIC")]
```

### Fitting an MA(2) model improved the AIC and BIC over the MA(1) model
```{r}
sarima(dl_varve,0,0,2, details=FALSE)[c("AIC", "BIC")]
```

### Fitting an ARMA(1,1) model improved the AIC and BIC over the MA(2) model
```{r}
sarima(dl_varve,1,0,1, details=FALSE)[c("AIC", "BIC")]
```

AIC and BIC help you find the model with the smallest error using the least number of parameters. The idea is based on the parsimony principle, which is basic to all science and tells you to choose the simplest scientific explanation that fits the evidence.

## Residual Analysis of models MA(1) and ARMA(1,1) for the Annual Varve log difference Series

### MA(1) model
```{r fig.height=7, fig.width=7}
sarima(dl_varve,0,0,1)
```

**Residual analysis:**

1. The standardized residuals should behave as a white noise sequence with mean zero and variance one. Examine the residual plot for departures from this behavior.

    It's hard to say.  It seems not so white.

2. The sample ACF of the residuals should look like that of white noise. Examine the ACF for departures from this behavior.

    There are 3 values outside the range, possibly not white noise.

3. Normality is an essential assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers.

    There's reasonable normality with few outliers.

4. Use the Q-statistic plot to help test for departures from whiteness of the residuals.

    All p-values are bellow the limit, confirming it there's still correlation in the residuals and this is not a good model fit.

### ARMA(1,1) model
```{r fig.height=7, fig.width=7}
sarima(dl_varve,1,0,1)
```

**Residual Analysis:**

1. The standardized residuals should behave as a white noise sequence with mean zero and variance one. Examine the residual plot for departures from this behavior.

    As in the previous case, it's hard to say.  It seems not so white.

2. The sample ACF of the residuals should look like that of white noise. Examine the ACF for departures from this behavior.

    All values are inside the range, possibly white noise.

3. Normality is an essential assumption when fitting ARMA models. Examine the Q-Q plot for departures from normality and to identify outliers.

    As in the previous case, there's reasonable normality with few outliers.

4. Use the Q-statistic plot to help test for departures from whiteness of the residuals.

    All p-values are above the limit, confirming there's no correlation in the residuals and this is a good model fit.

## Fitting a model to Crude oil, WTI spot price FOB

### Calculate approximate oil returns
```{r}
data(oil)
oil_returns <- diff(log(oil))
```

### Plot oil_returns.

Notice that there are a couple of outliers prior to 2004. Convince yourself that the returns are stationary.
```{r}
plot(cbind(oil, oil_returns), main="Crude oil, WTI spot price FOB ($/barrel), weekly data, 2000 to mid-2010")
```

### Plot the P/ACF pair for oil_returns
```{r}
acf2(oil_returns)
```

### Fit a model to oil_returns

From the P/ACF pair, it is apparent that the correlations are small and the returns are nearly noise. But it could be that both the ACF and PACF are tailing off. If this is the case, then an ARMA(1,1) is suggested. Fit this model to the oil returns using sarima().

```{r fig.height=7, fig.width=7}
sarima(oil_returns, 1,0,1)
```

It seems not a good fit because most of the p-values are bellow the limit.

# ARIMA - Integrated ARMA

A time series is called ARIMA(p,d,q) if the differenced series (of order d) is ARMA(p,q).

## Identifying ARIMA

A time series exhibits ARIMA behavior if the differenced data has ARMA behavior.

Simulation ARIMA(p = 1, d = 1, q = 0)
```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
par(mfrow=1:2)
plot(x, main = "ARIMA(p = 1, d = 1, q = 0)")
plot(diff(x), main = "ARMA(p = 1, d = 0, q = 0)")
```

## ACF and PACF of an Integrated ARMA

ACF decays in a linear fashion and PCF is almost 1 at lag one.

```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
acf2(x)
```

## ACF and PACF of a differenced ARMA

Indicates and ARMA model for the differenced data: ACF tails off, PACF cutsoff at lag one.

```{r}
x <- arima.sim(list(order = c(1, 1, 0), ar = .9), n = 200)
acf2(diff(x))
```

## Weekly Oil Prices

```{r}
par(mfrow=2:1)
plot(oil, main="Weekly Price of Oil: Random Walk")
plot(diff(oil), main="Differenced Price of Oil: Stationary")
invisible(acf2(diff(oil)))
```

Both ACF and PACF tail off, suggesting an ARMA(1,1) model on the differenced data or an ARIMA(1,1,1) on the data itself.

## ARIMA - Plug and Play

Simulated data from the integrated model $$Y_t=.9Y_t−1+W_t$$ where $Y_t=\nabla X_t=X_t−X_{t−1}$. In this case, the model is an ARIMA(1,1,0) because the differenced data are an autoregression of order one.

Plot x
```{r}
x <- arima.sim(model = list(order = c(1, 1, 0), ar = .9), n = 200)
plot(x)
```

Plot the P/ACF pair of x
```{r}
acf2(x)
```

Plot the differenced data
```{r}
plot(diff(x))
```

Plot the P/ACF pair of the differenced data

Note how they imply an AR(1) model for the differenced data.  ACF tails off and PACF cuts off.

```{r}
acf2(diff(x))
```

Differencing the data in your ARIMA(1,1,0) model makes it stationary and allows for further analysis

## Slightly more complicated simulated ARIMA

250 observations from the ARIMA(2,1,0) model with drift given by

$$Y_t=1+1.5Y_{t−1}−.75Y_{t−2}+W_t$$

where $Y_t=\nabla X_t=X_t−X_{t−1}$

```{r}
x <- arima.sim(model = list(order = c(2, 1, 0), ar = c(1.5, -.75)), n = 250, mean=1)
par(mfrow=2:1)
plot(x, main=expression(x[t]))
plot(diff(x), main=expression(paste(y[t], "=", x[t]-x[t-1])))
```

Plot sample P/ACF of differenced data and determine model
```{r}
invisible(acf2(diff(x)))
```

Estimate parameters and examine output
```{r}
sarima(x, 2,1,0, details=FALSE)$ttable
```

The estimated parameters are very close to 1.5 and -0.75

## Global Warming

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

Plot the sample P/ACF pair of the differenced data
```{r}
invisible(acf2(diff(globtemp)))
```

Either:

1. The ACF and the PACF are both tailing off, implying an ARIMA(1,1,1) model.

2. The ACF cuts off at lag 2, and the PACF is tailing off, implying an ARIMA(0,1,2) model.

3. The ACF is tailing off and the PACF cuts off at lag 3, implying an ARIMA(3,1,0) model. Although this model fits reasonably well, it is the worst of the three (you can check it) because it uses too many parameters for such small autocorrelations.

Fit an ARIMA(1,1,1) model to globtemp
```{r}
sarima(globtemp, 1,1,1, details = FALSE)[c("ttable", "AIC", "BIC")]
```
All parameters are significant.

Fit an ARIMA(0,1,2) model to globtemp
```{r}
sarima(globtemp, 0,1,2, details = FALSE)[c("ttable", "AIC", "BIC")]
```

All parameters are significant and both AIC and BIC shows this model is a better fit.

Fit an ARIMA(3,1,0) model to globtemp
```{r}
sarima(globtemp, 3,1,0, details = FALSE)[c("ttable", "AIC", "BIC")]
```

The drift estimate is not significant and AIC/BIC are worse than previous models.

# ARIMA Diagnostics

Diagnostics ARIMA models is the same as for ARMA models.

Once a model seems reasonable, try adding a parameter to see if it makes an improvement.  If it does so, change the model, otherwise the model is fitted.

## Weekly Oil Prices ARIMA(1,1,1)?

For this example, the Oil Series is subset until 2006.
```{r}
par(mfrow=2:1, mar=c(2,3,1,1))
data(oil)
plot(oil)
abline(v=2006, lty=2)
oil2006 <- window(oil, end = 2006)
plot(oil2006)
```

The P/ACF of the detrended oil price appears to tail off, suggesting an ARIMA(1,1,1)
```{r}
par(mfrow=2:1)
plot(oil2006, main="Weekly Price of Oil until 2006")
plot(diff(oil2006), main="Differenced Price of Oil until 2006")
invisible(acf2(diff(oil2006)))
```

```{r fig.height=7, fig.width=7}
sarima(oil2006, p = 1, d = 1, q = 1)["ttable"]
```

The parameter estimates $\phi$ and $\theta$ are all significant but the constant drift $\mu$.

The residual analysis looks fine too.

** Overfit: ARIMA(2, 1, 1) and ARIMA(1, 1, 2)**

We can try to add and AR parameter and an MA parameters to see it they make a difference.

```{r}
oil_fit1 <- sarima(oil2006, p = 2, d = 1, q = 1, details=FALSE)
oil_fit1$ttable
```

```{r}
oil_fit2 <- sarima(oil2006, p = 1, d = 1, q = 2, details=FALSE)
oil_fit2$ttable
```

We can see in each case the `ar2` and `ma2` parameters are not significant an the other former parameters are practically unchanged. Hence, the original model ARIMA(1,1,1) fits well.

## Diagnostics - Simulated Overfitting

One way to check an analysis is to overfit the model by adding an extra parameter to see if it makes a difference in the results. If adding parameters changes the results drastically, then you should rethink your model. If, however, the results do not change by much, you can be confident that your fit is correct.

Simulated 250 observations from an ARIMA(0,1,1) model with MA parameter .9
```{r}
x <- arima.sim(model=list(order=c(0,1,1), ma=.9), n=250)
plot(cbind(x, diff(x)), main=expression(paste("Simulated ARIMA(0,1,1) with ",theta," = .9")))
```

Plot sample P/ACF pair of the differenced data shows ACF cuts off lag 1 and PCF tails off, confirming an MA model.

```{r}
invisible(acf2(diff(x)))
```

Fit the first model, compare parameters, check diagnostics

```{r}
sarima(x, 0,1,1)
```

The ARIMA(0,1,1) easily identified with good parameter estimates.

Fit the second model adding a parameter: ARIMA(0,1,2)

```{r}
sarima(x,0,1,2)
```

As you can see from the t-table, the second MA parameter is not significantly different from zero and the first MA parameter is approximately the same in each run. Also, the AIC and BIC both increase when the parameter is added. In addition, the residual analysis of your ARIMA(0,1,1) model is fine. All of these facts together indicate that you have a successful model fit.

## Diagnostics - Global Temperatures

```{r}
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations")
```

Fit ARIMA(0,1,2) to globtemp and check diagnostics

```{r}
sarima(globtemp, 0,1,2)
```

Fit ARIMA(1,1,1) to globtemp and check diagnostics

```{r}
sarima(globtemp, 1,1,2)
```

The model diagnostics suggest that both the ARIMA(0,1,2) and the ARIMA(1,1,1) are reasonable models. However, the AIC and BIC suggest that the ARIMA(0,1,2) performs slightly better, so this should be your preferred model.

You can use overfitting to assess the final model. For example, try fitting an ARIMA(1,1,2) or an ARIMA(0,1,3) to the data.  The both show much higher AIC and BIC.

```{r}
sarima(x, 0,1,3)
```

```{r}
sarima(x, 1,1,2)
```

# Forecasting ARIMA

## Forecasting ARIMA Processes

* The model describes how the dynamics of the time series behave over time  
* Forecasting simply continues the model dynamics into the future  
* Use sarima.for() to forecast in the astsa-package.  It's similar to samira() but specifies the forescasting horizon

Forecast 52 weeks from 2006 on shown in red.  The dark grey area denotes +/- 1 Root Mean Squared prediction Error (RMSE) bounds and the light grey ribbon denotes +/- 2 RMSE bounds which represents 95% prediction interval.
```{r}
data(oil)
oil2006 <- window(oil, end = 2006)
oil2007 <- window(oil, end = 2007)
invisible(sarima.for(oil2006, n.ahead = 52, 1, 1, 1))
lines(oil2007, lty=3)
```

## Forecasting Simulated ARIMA

Simulated 120 observations from an ARIMA(1,1,0) model with AR parameter .9.
```{r}
y <- arima.sim(model=list(order=c(1,1,0), ar=.9), n=120)
```
Taking the first 100 observations for model fitting.
```{r}
x <- window(y, end=100)
plot(cbind(x, diff(x)), main="Simulated ARIMA(1,1,0) AR=.9")
```

Plot P/ACF pair of differenced data show ACF tails off and PACF cuts off lag 1 confirming AR(1)
```{r}
invisible(acf2(diff(x)))
```

Fitting the AR(1,1,0) model and checking t-table and diagnostics confirm parameters estimate.
```{r}
sarima(x, 1,1,0)
```

Forecast the data 20 time periods ahead

```{r}
invisible(sarima.for(x, n.ahead = 20, p = 1, d = 1, q = 0) )
lines(y)
```

## Forecasting Global Temperatures

Forecasting the annual global temperature deviations globtemp to 2050.

```{r}
data(globtemp)
plot(cbind(globtemp, diff(globtemp)), main="Global mean land-ocean temperature deviations until 2015")
```

As checked before, ARIMA(0,1,2) is the best fit for this data.  It can be re-checked by fitting an ARIMA(0,1,2) to globtemp.
```{r}
sarima(globtemp, 0,1,2)
```

Forecasting data 35 years into the future
```{r}
sarima.for(globtemp, 35, 0,1,2)
```

