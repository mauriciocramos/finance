---
title: "Time Series Basics"
author: "Maurício Collaça"
date: "August 5, 2018"
output: 
  html_document: 
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# A random normal series
```{r}
set.seed(123)
t <- seq(from = 1, to = 100, by = 1) + 10 + rnorm(100, sd = 7)
plot(t, type="l")
```

Normality test
```{r}
shapiro.test(t) 
```

Histogram
```{r}
hist(t, prob=TRUE)    
lines(density(t))
```

Normal Q-Q plot
```{r}
qqnorm(t)  
qqline(t)
```

# Time Series Objects

## ts() function arguments
```{r}
args(ts)
```

## Create quarterly data from the first quarter of 2000 on
```{r}
(tseries <- ts(t, start = c(2000, 1), frequency = 4))
```

## plot(), actually plot.ts()
```{r}
plot(tseries)
```

## Multiple series.
```{r}
set.seed(123)
seq <- seq(from = 1, to = 25, by = 1) + 10
ts1 <- seq + rnorm(25, sd = 5)
ts2 <- seq + rnorm(25, sd = 12)
ts3 <- seq^2 + rnorm(25, sd = 300)
tsm <- cbind(ts1, ts2, ts3)
(tsm <- ts(tsm, start=c(2000, 1), frequency = 4))
```

## plot multiple series by default
```{r}
plot(tsm)
```

## plot.ts() arguments
```{r}
args(plot.ts)
```

## plot multiple series in single panel
```{r}
plot(tsm[,1:2], plot.type=c("single"), col=1:2)
```

# Convenience Functions for Time Series

## start() and end() - terminal times
```{r}
start(tseries)
end(tseries)
```

## window() - subset by time

Only the data from the first quarter of 2000 to the last quarter of 2012
```{r}
(tseries_sub <- window(tseries, start=c(2000, 1), end=c(2012,4)))
```

## frequency() - Number of samples per unit time
```{r}
frequency(tseries)
```

## cycle() - Positions in the cycle of each observation
```{r}
cycle(tseries)
```

## deltat() - Interval time between observations
```{r}
deltat(tseries)
```

## time() - Sampling times
```{r}
time(tseries)
```

# Time Series transformations

There are a number of different functions that can be used to transform time series data such as the difference, log, moving average, percent change, lag, or cumulative sum.

For example, the moving average function can be used to more easily visualize a high-variance time series and is also a critical part the ARIMA family of models. Functions such as the difference, percent change, and log difference are helpful for making non-stationary data stationary.

## lag()

Compute a lagged version of a time series, shifting the time base back (positive lag) or forth (negative lag) by a given number of observations

Lag 1 unit of observation
```{r}
tseries_lag1 <- lag(tseries, 1)
head(cbind(tseries, tseries_lag1))
```

Lag 3 units of observation
```{r}
tseries_lag3 <- lag(tseries, 3)
head(cbind(tseries, tseries_lag3))
```

Lead 1 unit of observation
```{r}
tseries_lead1 <- lag(tseries, -1)
head(cbind(tseries, tseries_lead1))
```

## diff(): remove trend & make stationary

Difference between each value and its previous value (lag 1)
```{r}
tseries_diff1 <- diff(tseries, lag = 1)
tsd1 <- cbind(tseries, tseries_diff1)
head(tsd1)
```

Taking a difference is an effective way to remove a trend and make a time series stationary
```{r}
plot(tsd1, main="Difference (lag=1)")
```

Difference between each value and its before previous value (lag 2)
```{r}
tseries_diff2 <- diff(tseries, lag = 2)
tsd2 <- cbind(tseries, tseries_diff2)
head(tsd2)
```

```{r}
plot(tsd2, main="Difference (lag=2)")
```

## Heteroskedasticity: non-constant (e.g. increasing) variance

The prevalence of increasing, or more generally non-constant, variance is called heteroskedasticity, i.e., sub-populations having difference variance from others, and can cause problems in linear regression. Often, it will need to be corrected before modeling.

Simulating a time series with increasing variance:
```{r}
trend <- ts(seq(from = 10, to = 110))
cycle <- ts(sin(trend)) * 0.2 * trend
tseries_h <- trend + cycle
plot(cbind(trend, cycle, "trend + cycle"=tseries_h), main="Simulated time series with increasing variance")
```

Some time series transformation functions are useful for series in which the variance gets larger over time. These range from the *basic logarithm function* to the *Box-Cox group of transformations* (of which the natural logarithm is a special case).

## log(): corrects heteroskedasticity

```{r}
plot(cbind(tseries_h, log(tseries_h)), main="Log transformation")
```

## BoxCox() transformation: corrects heteroskedasticity

There are other more advanced ways of eliminating non-constant variance (heteroskedasticity), one of which is the **Box-Cox transformation**, which allows us a bit more control over the transformation. The Box-Cox takes the form (Hyndman and Athanasopoulos, 2013):

$$w_t = \begin{cases} \log{ y_t }, \text{ if } \lambda = 0; \\ \frac{ ({y_t}^{\lambda} – 1) }{ \lambda },\text{ otherwise } \\ \end{cases} $$

```{r}
library(forecast)
```

It has two functions that are of use here. The primary function is BoxCox(), which will return a transformed time series given a time series and a value for the parameter lambda:

```{r}
plot.ts(cbind(tseries_h, "BoxCox(0.5)"=BoxCox(tseries_h, lambda = 0.5)), main="Box-Cox transformation (lambda=0.5)")
```

Notice that this value **0.5** of lambda here does not entirely take care of the heteroskedasticity problem. We can experiment with different values of lambda, or we can use the **BoxCox.lambda()** function, which will provide us an optimal value for parameter lambda:

```{r}
(lambda <- BoxCox.lambda(tseries_h))
```

The BoxCox.lambda() function has chosen the value 0.055. If we then use this value in our BoxCox() function, it returns a time series that appears to have constant variance.

```{r}
plot.ts(cbind(tseries_h, "BoxCox(0.05)"=BoxCox(tseries_h, lambda = lambda)), main="Box-Cox transformation (lambda=0.05)")
```

## Percent change: remove trend & make stationary

Another common calculation that we may want to perform on time series is the **percent change** from one period to another. A function to easily calculate percent change with an argument to specify the number of periods over which we want to calculate the change, defaults 1.

```{r}
pch <- function(data, lag = 1) {
    if (!is.ts(data)) stop("data must be of type ts")
    if (!is.numeric(lag)) stop("lag must be of type numeric")
    data / lag(data, -lag) - 1
}
```

Quarterly percent change
```{r}
pch_quarter <- pch(tseries)
head(cbind(tseries, pch_quarter))
```
```{r}
plot(cbind(tseries, pch_quarter), main="Percent Change Tranformation (lag=1)")
```

Year over year percent change
```{r}
pch_year <- pch(tseries, lag=4)
head(cbind(tseries, pch_year))
```
```{r}
plot(cbind(tseries, pch_year), main="Percent Change Tranformation (lag=4)")
```

## Log difference: remove trend & make stationary

Two of the functions that we have discussed so far, the difference and the log, are often combined in time series analysis. The **log difference** function is useful for making non-stationary data stationary and has some other useful properties.

```{r}
tseries_logdiff <- diff(log(tseries))
plot.ts(cbind(tseries, tseries_logdiff), main="Log difference (lag=1) transformation")
```

Notice that after taking the log return, tseries appears to be stationary. We see some higher than normal volatility in the beginning of the series. This is due largely to the fact that the series levels start off so small. This leads into a nice property of the **log diference return function: a close approximation to the percent change**:

```{r}
plot.ts(cbind(pch_quarter, tseries_logdiff),
        main="log diference: a close approximation to the percent change")
```

This similarity is only approximate. The relationship does break down somewhat when the percent change from one period to the next is particularly large. You can read a good discussion of this topic [here](https://stats.stackexchange.com/questions/244199/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th).

## Moving average: Smoothing/trending

A **moving average** is another essential function for working with time series. For series with particularly high volatility, a moving average can help us to more clearly visualize its trend. We can use base R’s filter() function, which allows us to perform general linear filtering. We can set up the parameters of this function to be a moving average (Shumway and Stoffer, 2011). Here we apply the filter() function to tseries to create a 5 period moving average. The filter argument lets up specify the filter, which in this case is just a weighted average of 5 observations. The sides argument allows us to specify whether we want to apply the filter over past values (sides = 1), or to both past and future values (sides = 2).

```{r}
tseries_lf5 <- filter(tseries, filter = rep(1/5, 5), sides = 1)
head(cbind(tseries, tseries_lf5))
```

```{r}
plot.ts(cbind(tseries, tseries_lf5), plot.type='single', col=1:2,
        main="Moving Average n-5 window")
```

The fact that we have to define the linear filter each time we use this function makes it a little cumbersome to use. If we don’t mind introducing a dependency to our code, we could use the `SMA()` function from the `TTR` package or the `ma()` function from the `forecast` package. The SMA() function takes a ts object and a value for n – the window over which we want to calculate the moving average.

```{r}
library(TTR)
tseries_ma5 <- SMA(tseries, n = 5)
```

The `ma()` function from the `forecast` package also performs moving average calculations. We supply the time series and a value for the order argument.

```{r}
tseries_ma5fore <- ma(tseries, order = 5)
```

Let’s compare the results. The SMA() function returns a trailing moving average where each value is the mean of the n most recent trailing values. This is equivalent to the results we get from using the `filter()` function. The `ma()` function from the forecast package returns a centered moving average. In this case tseries_ma5for is equal to the average of the current observation, the previous two observation, and the next two observations. Which one you use would depend on your application.

```{r}
head(cbind(tseries, tseries_lf5, tseries_ma5, tseries_ma5fore),10)
```

# Real data with Quandl API

Quandl has a great warehouse of financial, economic and alternative data, some of which is free.

Below is an example of using the Quandl R package to get housing price index data. You can find this data on the web [here](https://www.quandl.com/data/YALE/NHPI-Historical-Housing-Market-Data-Nominal-Home-Price-Index).

```{r}
library(Quandl)
```
```{r}
Quandl.api_key(readLines("Quandl.api_key.txt",1))
hpidata <- Quandl("YALE/NHPI", type="ts", start_date="1990-01-01")
plot(hpidata, main = "Robert Shiller's Nominal Home Price Index")
```

Data on US GDP and US personal income, and the University of Michigan Consumer Survey on selling conditions for houses. The data are located on the web [here](https://www.quandl.com/data/FRED/GDP-Gross-Domestic-Product), [here](https://www.quandl.com/data/FRED/PINCOME-Personal-Income), and [here](https://www.quandl.com/data/UMICH/SOC43-University-of-Michigan-Consumer-Survey-Selling-Conditions-for-Houses).

```{r}
gdpdata <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01")
pidata <- Quandl("FRED/PINCOME", type="ts", start_date="1990-01-01")
umdata <- Quandl("UMICH/SOC43", type="ts")[, 1]
plot.ts(cbind(gdpdata, pidata),  main="US GPD and Personal Income, billions $")
```

```{r}
plot.ts(umdata, main = "UM Consumer Survey, House selling conditions")
```

The Quandl API also has some basic options for data preprocessing. The US GDP data is in quarterly frequency...
```{r}
frequency(gdpdata)
```
... but assume we want annual data. We can use the collapse argument to collapse the data to a lower frequency. Here we covert the data to annual as we import it.

```{r}
gdpdata_ann <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01", collapse="annual")
frequency(gdpdata_ann)
```

We can also transform our data on the fly as its imported. The Quandl function has a argument transform that allows us to specify the type of data transformation we want to perform. There are five options – “diff“, ”rdiff“, ”normalize“, ”cumul“, ”rdiff_from“. Specifying the transform argument as”diff” returns the simple difference, “rdiff” yields the percentage change, “normalize” gives an index where each value is that value divided by the first period value and multiplied by 100, “cumul” gives the cumulative sum, and “rdiff_from” gives each value as the percent difference between itself and the last value in the series. For more details on these transformations, check the API documentation [here](https://docs.quandl.com/).

For example, here we get the data in percent change form:
```{r}
gdpdata_pc <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01", transform="rdiff")
plot(gdpdata_pc * 100, ylab= "% change", main="US Gross Domestic Product, % change")
```

You can find additional documentation on using the Quandl R package [here](https://docs.quandl.com/docs/r-time-series). The API allows a maximum of 50 calls per day from anonymous users. You can sign up for an account and get your own API key, which will allow you to make as many calls to the API as you like (within reason of course).