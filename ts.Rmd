---
title: "Time Series Basics"
author: "Maurício Collaça"
date: "August 5, 2018"
output: 
  html_document: 
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
options(width = 100)
```

# A random normal series
```{r}
set.seed(123)
t <- seq(from = 1, to = 100, by = 1) + 10 + rnorm(100, sd = 7)
plot(t, type="l")
```

Normality test
```{r}
shapiro.test(t) 
```

Histogram
```{r}
hist(t, prob=TRUE)    
lines(density(t))
```

Normal Q-Q plot
```{r}
qqnorm(t)  
qqline(t)
```

# Time Series Objects

## ts() function arguments
```{r}
args(ts)
```

## Create quarterly data from the first quarter of 2000 on
```{r}
(tseries <- ts(t, start = c(2000, 1), frequency = 4))
```

## plot(), actually plot.ts()
```{r}
plot(tseries)
```

## Multiple series.
```{r}
set.seed(123)
seq <- seq(from = 1, to = 25, by = 1) + 10
ts1 <- seq + rnorm(25, sd = 5)
ts2 <- seq + rnorm(25, sd = 12)
ts3 <- seq^2 + rnorm(25, sd = 300)
tsm <- cbind(ts1, ts2, ts3)
(tsm <- ts(tsm, start=c(2000, 1), frequency = 4))
```

## plot multiple series by default
```{r}
plot(tsm)
```

## plot.ts() arguments
```{r}
args(plot.ts)
```

## plot multiple series in single panel
```{r}
plot(tsm[,1:2], plot.type=c("single"), col=1:2)
```

# Convenience Functions for Time Series

## start() and end() - terminal times
```{r}
start(tseries)
end(tseries)
```

## window() - subset by time

Only the data from the first quarter of 2000 to the last quarter of 2012
```{r}
(tseries_sub <- window(tseries, start=c(2000, 1), end=c(2012,4)))
```

## frequency() - Number of samples per unit time
```{r}
frequency(tseries)
```

## cycle() - Positions in the cycle of each observation
```{r}
cycle(tseries)
```

## deltat() - Interval time between observations
```{r}
deltat(tseries)
```

## time() - Sampling times
```{r}
time(tseries)
```

# Time Series transformations

There are a number of different functions that can be used to transform time series data such as the difference, log, moving average, percent change, lag, or cumulative sum.

For example, the moving average function can be used to more easily visualize a high-variance time series and is also a critical part the ARIMA family of models. Functions such as the difference, percent change, and log difference are helpful for making non-stationary data stationary.

## Lag

Compute a lagged version of a time series, shifting the time base back (positive lag) or forth (negative lag) by a given number of observations

Lag 1 unit of observation
```{r}
tseries_lag1 <- lag(tseries, 1)
head(cbind(tseries, tseries_lag1))
```

Lag 3 units of observation
```{r}
tseries_lag3 <- lag(tseries, 3)
head(cbind(tseries, tseries_lag3))
```

Lead 1 unit of observation
```{r}
tseries_lead1 <- lag(tseries, -1)
head(cbind(tseries, tseries_lead1))
```

## Difference

diff() removes a linear trend, producing a so called change series or difference series

### Removing a linear trend

Difference between each value and its previous value (lag 1)
```{r}
tseries_diff1 <- diff(tseries, lag = 1)
tsd1 <- cbind(tseries, tseries_diff1)
head(tsd1)
```

Taking a difference is an effective way to remove a trend and make a time series stationary
```{r}
plot(tsd1, main="Removing linear trend")
```

### Removing a seasonal trend

```{r include=FALSE}
tseries_season4 <- ts(c(-4.19803335973822, 9.56900905050747, 5.17514338906622, -9.69164636184119, 
-3.21529357476552, 10.8436693062559, 6.45215865708188, -10.8335590731441, -2.23535073514566, 10.119832962947, 6.57964573528545, -8.65656473226148, -2.51500138847406, 9.83743416797243, 7.38619432726046, -8.24350355631342, 
-4.26403257072438, 8.89886143008727, 8.54433581556254, -8.06691284107053, -4.02302530566536, 9.82267856454484, 7.77285238609707, -6.58777686807675, -3.45917112006633, 10.6138505016733, 7.37445047382904, -5.79871450522201, 
-1.20471055826558, 11.4292358845515, 7.57004694873634, -4.96838375977415, -2.00378659668805, 11.9413482982078, 9.40667210265289, -4.39658482425492, -1.55557889273259, 12.5998771289623, 8.50291637845889, -3.72896826710606, 
-2.82700000921691, 13.3759812867942, 8.12894114749321, -3.14924907770406, -2.79947272334396, 13.7105701226795, 6.75521688156725, -3.77974417787649, -3.76827394084617, 13.6253358560593, 6.53793056349892, -3.24909834265677, 
-5.02419102826334, 13.3553731337997, 6.93116080641335, -3.52735350362993, -5.19732896878058, 11.5797913233393, 7.16244857247412, -1.89460681643879, -5.77779700690048, 12.4826952884178, 6.20808841277938, -3.43403797043816, 
-7.08072100968159, 11.4136557746633, 6.74199013667799, -3.53237586423137, -8.39354230059932, 12.5072611286297, 6.47317547164243, -3.74524618197473, -9.42620870823793, 12.3808170457897, 8.04824272204917, -2.83152774381656, 
-7.30189285880787, 12.7658383061004, 8.22369850629786, -4.44813067987144, -6.96355759602497, 12.0340049862918, 7.57492470964176, -5.40221818593254, -6.56819814480662, 10.8964815061608, 7.27657057973042, -4.03787337902209, 
-6.72301328397502, 12.1808149692035, 8.28516213832065, -4.15934247966369, -6.36067044646075, 12.7530177649457, 8.66591165178255, -5.44053784032607, -4.87493170537183, 12.6001974041011, 8.16258920594197, -6.53957167229621))
```
```{r}
tseries_diff4 <- diff(tseries_season4, lag=4)
plot.ts(cbind(tseries_season4, tseries_diff4), nc=2, main="Removing seasonal trends")
```

## Heteroskedasticity: non-constant (e.g. increasing) variance

The prevalence of increasing, or more generally non-constant, variance is called heteroskedasticity, i.e., sub-populations having difference variance from others, and can cause problems in linear regression. Often, it will need to be corrected before modeling.

Simulating a time series with increasing variance:
```{r}
trend <- ts(seq(from = 10, to = 110))
cycle <- ts(sin(trend)) * 0.2 * trend
tseries_h <- trend + cycle
plot(cbind(trend, cycle, "trend + cycle"=tseries_h), main="Simulated time series with increasing variance")
```

Some time series transformation functions are useful for series in which the variance gets larger over time. These range from the *basic logarithm function* to the *Box-Cox group of transformations* (of which the natural logarithm is a special case).

## Log transformation of a series

### Correcting heteroskedasticity

```{r}
plot(cbind(tseries_h, log(tseries_h)), nc=2, main="Log transformation of the heteroskedasticity")
```

### Linearizing a rapid growth trend
```{r include=FALSE}
rapid_growth <- c(505.954737316542, 447.35560816695, 542.583090477044, 516.063361274983, 506.959924087797, 535.016232606015, 496.929129437996, 497.562555328502, 
577.248300530956, 536.855967325257, 541.245940412262, 473.497840289097, 550.988961460551, 569.410637437069, 522.915183368473, 487.200195130919, 
594.610786395524, 591.173975668723, 615.986775394564, 621.317545120693, 607.125046916761, 587.036701378981, 554.155438462285, 644.117241827027, 
509.700025728118, 607.094339401359, 603.55118553444, 613.621586650076, 544.914334775047, 670.811820766561, 687.13161588209, 615.581708859918, 
711.187318608945, 694.297903386338, 681.929344495129, 659.140256189321, 642.702057830158, 601.530062712521, 666.762307473106, 650.965683100529, 
606.091343627865, 696.678827373702, 641.602509472816, 855.771919797152, 667.329074652784, 573.491390192462, 791.733265065015, 751.591446783947, 
610.794814080548, 624.65033623201, 833.298991700858, 639.886724484288, 736.828256803519, 772.292341071224, 686.886519321095, 667.763083378498, 
712.94149525726, 918.183828402689, 656.108907724902, 700.497201964217, 683.493272462585, 781.737968201983, 715.684280499056, 808.287549436184, 
820.779546770362, 656.885589359776, 733.339974340827, 773.538730437607, 641.202740082118, 932.21185739678, 680.676565427109, 988.282815285029, 
664.898568280288, 813.528318290545, 883.408798971726, 924.274924691176, 969.43211824107, 777.329346514109, 880.998409919801, 971.358289510766, 
902.958423571348, 1020.74565895517, 1075.14831710597, 886.170748719307, 889.632200892515, 950.390776999556, 878.039522146964, 1043.76763270868, 
901.108966040025, 1079.65840856798, 933.905398508361, 921.9433013918, 870.807113616954, 811.139766624589, 1004.26766243592, 1008.17582581146, 
1189.48933735193, 751.970606760973, 947.475252330288, 886.515305193286, 1074.89426408969, 1101.1306720708, 1130.1854576199, 975.849507265787, 
948.161009999662, 1177.8226568287, 1227.12713297946, 976.995711035094, 836.708871866655, 1323.60473389093, 852.35320730904, 1200.82615703738, 
1274.47884753083, 1349.2613900482, 1102.63337196778, 1324.85659429696, 1268.71865481688, 1058.22892886463, 1204.08723498947, 1084.65028499166, 
1284.43050935569, 1195.28425174124, 1058.42621256846, 1188.0576623439, 1166.59336076, 1064.69458969538, 1429.06852442551, 1070.85280871704, 
1539.33053352377, 1467.15710870044, 1127.70582024314, 1296.0717171226, 1555.27412822535, 1332.90372372162, 1315.4236396165, 1189.2461691901, 
1482.43394167998, 1240.92868932515, 1237.77196935431, 1468.60833835437, 1328.54569942606, 1589.50783787689, 1373.16295850591, 1503.55627838567, 
1659.93756371734, 1704.6136574176, 1550.46378217986, 1625.80261162794, 1873.85823632632, 1370.62086282574, 1439.71135339554, 1447.43691659102, 
1579.91576345679, 1681.2571135903, 1661.60589978392, 1311.84684158442, 1326.03084218461, 1323.09951189813, 1550.48631755657, 1606.20420490403, 
1768.54014627508, 1509.8367966576, 1592.10861892944, 1627.61875806856, 1544.63289803997, 1439.52343145633, 1682.35177380745, 1850.70970808186, 
1673.38009271644, 1832.42716167314, 1672.26717594592, 1781.57683020519, 1659.28985001442, 1970.03886201406, 2044.7123676587, 1929.09019953541, 
1891.7042228956, 1487.15774065956, 2013.8721752695, 1796.78863307401, 1977.01825843918, 1516.95521151591, 1650.6039164404, 1523.28339404562, 
1696.6181065012, 1627.26087609279, 1787.29683278825, 1567.28735420676, 1881.99627254267, 2318.98327490681, 1941.98786180548, 1820.27972976969, 
2154.81229145692, 2261.54706560833, 2052.22139601365, 2079.17104479186, 2010.06086639259, 2145.26059886687, 1775.30080127631, 2013.40704947701)
```
```{r}
plot.ts(cbind(rapid_growth, linear_growth=log(rapid_growth)), plot.type = "multiple", nc=2,
        main="Log transformation of a rapid linear growth")
```

## BoxCox() transformation: corrects heteroskedasticity

There are other more advanced ways of eliminating non-constant variance (heteroskedasticity), one of which is the **Box-Cox transformation**, which allows us a bit more control over the transformation. The Box-Cox takes the form (Hyndman and Athanasopoulos, 2013):

$$w_t = \begin{cases} \log{ y_t }, \text{ if } \lambda = 0; \\ \frac{ ({y_t}^{\lambda} – 1) }{ \lambda },\text{ otherwise } \\ \end{cases} $$

```{r}
library(forecast)
```

It has two functions that are of use here. The primary function is BoxCox(), which will return a transformed time series given a time series and a value for the parameter lambda:

```{r}
plot.ts(cbind(tseries_h, "BoxCox(0.5)"=BoxCox(tseries_h, lambda = 0.5)), main="Box-Cox transformation (lambda=0.5)")
```

Notice that this value **0.5** of lambda here does not entirely take care of the heteroskedasticity problem. We can experiment with different values of lambda, or we can use the **BoxCox.lambda()** function, which will provide us an optimal value for parameter lambda:

```{r}
(lambda <- BoxCox.lambda(tseries_h))
```

The BoxCox.lambda() function has chosen the value 0.055. If we then use this value in our BoxCox() function, it returns a time series that appears to have constant variance.

```{r}
plot.ts(cbind(tseries_h, "BoxCox(0.05)"=BoxCox(tseries_h, lambda = lambda)), main="Box-Cox transformation (lambda=0.05)")
```

## Percent change: remove trend & make stationary

Another common calculation that we may want to perform on time series is the **percent change** from one period to another. A function to easily calculate percent change with an argument to specify the number of periods over which we want to calculate the change, defaults 1.

```{r}
pch <- function(data, lag = 1) {
    if (!is.ts(data)) stop("data must be of type ts")
    if (!is.numeric(lag)) stop("lag must be of type numeric")
    data / lag(data, -lag) - 1
}
```

Quarterly percent change
```{r}
pch_quarter <- pch(tseries)
head(cbind(tseries, pch_quarter))
```
```{r}
plot(cbind(tseries, pch_quarter), main="Percent Change Tranformation (lag=1)")
```

Year over year percent change
```{r}
pch_year <- pch(tseries, lag=4)
head(cbind(tseries, pch_year))
```
```{r}
plot(cbind(tseries, pch_year), main="Percent Change Tranformation (lag=4)")
```

## Log difference: remove trend & make stationary

Two of the functions that we have discussed so far, the difference and the log, are often combined in time series analysis. The **log difference** function is useful for making non-stationary data stationary and has some other useful properties.

```{r}
tseries_logdiff <- diff(log(tseries))
plot.ts(cbind(tseries, tseries_logdiff), main="Log difference (lag=1) transformation")
```

Notice that after taking the log return, tseries appears to be stationary. We see some higher than normal volatility in the beginning of the series. This is due largely to the fact that the series levels start off so small. This leads into a nice property of the **log diference return function: a close approximation to the percent change**:

```{r}
plot.ts(cbind(pch_quarter, tseries_logdiff),
        main="log diference: a close approximation to the percent change")
```

This similarity is only approximate. The relationship does break down somewhat when the percent change from one period to the next is particularly large. You can read a good discussion of this topic [here](https://stats.stackexchange.com/questions/244199/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th).

## Moving average: Smoothing/trending

A **moving average** is another essential function for working with time series. For series with particularly high volatility, a moving average can help us to more clearly visualize its trend. We can use base R’s filter() function, which allows us to perform general linear filtering. We can set up the parameters of this function to be a moving average (Shumway and Stoffer, 2011). Here we apply the filter() function to tseries to create a 5 period moving average. The filter argument lets up specify the filter, which in this case is just a weighted average of 5 observations. The sides argument allows us to specify whether we want to apply the filter over past values (sides = 1), or to both past and future values (sides = 2).

```{r}
tseries_lf5 <- filter(tseries, filter = rep(1/5, 5), sides = 1)
head(cbind(tseries, tseries_lf5))
```

```{r}
plot.ts(cbind(tseries, tseries_lf5), plot.type='single', col=1:2,
        main="Moving Average n-5 window")
```

The fact that we have to define the linear filter each time we use this function makes it a little cumbersome to use. If we don’t mind introducing a dependency to our code, we could use the `SMA()` function from the `TTR` package or the `ma()` function from the `forecast` package. The SMA() function takes a ts object and a value for n – the window over which we want to calculate the moving average.

```{r}
library(TTR)
tseries_ma5 <- SMA(tseries, n = 5)
```

The `ma()` function from the `forecast` package also performs moving average calculations. We supply the time series and a value for the order argument.

```{r}
tseries_ma5fore <- ma(tseries, order = 5)
```

Let’s compare the results. The SMA() function returns a trailing moving average where each value is the mean of the n most recent trailing values. This is equivalent to the results we get from using the `filter()` function. The `ma()` function from the forecast package returns a centered moving average. In this case tseries_ma5for is equal to the average of the current observation, the previous two observation, and the next two observations. Which one you use would depend on your application.

```{r}
head(cbind(tseries, tseries_lf5, tseries_ma5, tseries_ma5fore),10)
```

# Real data with Quandl API

Quandl has a great warehouse of financial, economic and alternative data, some of which is free.

Below is an example of using the Quandl R package to get housing price index data. You can find this data on the web [here](https://www.quandl.com/data/YALE/NHPI-Historical-Housing-Market-Data-Nominal-Home-Price-Index).

```{r}
library(Quandl)
```
```{r}
Quandl.api_key(readLines("Quandl.api_key.txt",1))
hpidata <- Quandl("YALE/NHPI", type="ts", start_date="1990-01-01")
plot(hpidata, main = "Robert Shiller's Nominal Home Price Index")
```

Data on US GDP and US personal income, and the University of Michigan Consumer Survey on selling conditions for houses. The data are located on the web [here](https://www.quandl.com/data/FRED/GDP-Gross-Domestic-Product), [here](https://www.quandl.com/data/FRED/PINCOME-Personal-Income), and [here](https://www.quandl.com/data/UMICH/SOC43-University-of-Michigan-Consumer-Survey-Selling-Conditions-for-Houses).

```{r}
gdpdata <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01")
pidata <- Quandl("FRED/PINCOME", type="ts", start_date="1990-01-01")
umdata <- Quandl("UMICH/SOC43", type="ts")[, 1]
plot.ts(cbind(gdpdata, pidata),  main="US GPD and Personal Income, billions $")
```

```{r}
plot.ts(umdata, main = "UM Consumer Survey, House selling conditions")
```

The Quandl API also has some basic options for data preprocessing. The US GDP data is in quarterly frequency...
```{r}
frequency(gdpdata)
```
... but assume we want annual data. We can use the collapse argument to collapse the data to a lower frequency. Here we covert the data to annual as we import it.

```{r}
gdpdata_ann <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01", collapse="annual")
frequency(gdpdata_ann)
```

We can also transform our data on the fly as its imported. The Quandl function has a argument transform that allows us to specify the type of data transformation we want to perform. There are five options – “diff“, ”rdiff“, ”normalize“, ”cumul“, ”rdiff_from“. Specifying the transform argument as”diff” returns the simple difference, “rdiff” yields the percentage change, “normalize” gives an index where each value is that value divided by the first period value and multiplied by 100, “cumul” gives the cumulative sum, and “rdiff_from” gives each value as the percent difference between itself and the last value in the series. For more details on these transformations, check the API documentation [here](https://docs.quandl.com/).

For example, here we get the data in percent change form:
```{r}
gdpdata_pc <- Quandl("FRED/GDP", type="ts", start_date="1990-01-01", transform="rdiff")
plot(gdpdata_pc * 100, ylab= "% change", main="US Gross Domestic Product, % change")
```

You can find additional documentation on using the Quandl R package [here](https://docs.quandl.com/docs/r-time-series). The API allows a maximum of 50 calls per day from anonymous users. You can sign up for an account and get your own API key, which will allow you to make as many calls to the API as you like (within reason of course).

# Time indexing

## What does the time index tell us?

Some data are naturally evenly spaced by time. Discrete time indexing is appropriate for discrete_data.
```{r}
discrete_data <- c(0.479425538604203, 0.841470984807897, 0.997494986604054, 0.909297426825682, 0.598472144103957, 0.141120008059867, -0.35078322768962, -0.756802495307928, -0.977530117665097, -0.958924274663138, -0.705540325570392, -0.279415498198926, 0.215119988087816, 0.656986598718789, 0.937999976774739, 0.989358246623382, 0.79848711262349, 0.412118485241757, -0.0751511204618093, -0.54402111088937)
discrete_time_index <- 1:20
plot(discrete_time_index, discrete_data, type="b", xaxt="n")
axis(1, discrete_time_index)
```

The next time series continuous_series also has 20 observations, it is following the same periodic pattern as discrete_data, but its observations are not evenly spaced. Continuous time indexing is natural for continuous_series, however, the observations are approximately evenly spaced, with about 1 observation observed per time unit.

```{r}
continuos_data <- c(0.568894675391674, 0.766304082748762, 0.992075122074743, 0.974817414246897, 0.399123202687807, 0.37660246288782, -0.385320332818421, -0.836358518142533, -0.999669829045622, -0.998310189536195, -0.646222799842192, -0.0938615130061447, 0.400529093874378, 0.681605778899236,0.953181592214187, 0.996938027205692, 0.839341938468838, 0.370037544609373, -0.255096756784655, -0.617439825310109)
continuous_time_index <- c(1.21032244665548, 1.74613730167039, 2.88963444461115, 3.59138367255218, 5.46206454467028, 5.51093333004974, 7.0742951775901, 8.26439798949286, 9.37338230921887, 9.5410633541178, 11.1611216005404, 12.378370850347, 13.3905590295326, 14.0662804655731, 15.0935473444406, 15.8645145166665, 16.8574128514156, 18.0914570542518, 19.3654514602385, 20.1805236600339)
plot(continuous_time_index, continuos_data, type="b", xaxt="n")
axis(1, round(continuous_time_index,1))
```

Let's investigate using a discrete time indexing for continuous_data.

```{r}
plot(discrete_time_index, continuos_data, type="b", xaxt="n")
lines(loess(continuos_data ~ continuous_time_index), col=2)
axis(1, discrete_time_index)
```

Note the various differences between the resulting figures, but the approximation appears reasonable because the overall trend remained preserved.

## Sampling Frequency

* Some time series data is exactly evenly spaced  
* Some time series data is only approximately evenly spaced  
* Some time series data is evenly spaced, but with missing values

Simplifying assumptions for time series:

* Consecutive observations are equally spaced  
* Apply a discrete-time observation index  
* This may only hold approximately

Ex. Daily log returns on stock may only be a vailable for weekdays.  
Ex. Monthly CPI values are equally spaced by month, not by day.

# White Noise (WN)

White Noise (WN) is the simplest example of a stationary process.

A weak white noise process has:

* A fixed, constant mean  
* A fixed, constant variance  
* No correlation over time
* i.e. Idependent and Identically Distributed (IID) variables

The white noise (WN) model is a basic time series model. It is also a basis for the more elaborate models we will consider. We will focus on the simplest form of WN, independent and identically distributed data.

The arima.sim() function can be used to simulate data from a variety of time series models. ARIMA is an abbreviation for the autoregressive integrated moving average class of models we will consider throughout this course.

An ARIMA(p, d, q) model has three parts, the autoregressive order p, the order of integration (or differencing) d, and the moving average order q. ARIMA(0, 0, 0) model, i.e., with all of these components zero, is simply the WN model.

## Simulate a WN model with $\mu$=0 and $\sigma$=1
```{r}
white_noise <- arima.sim(model = list(order=c(0,0,0)), n = 100)
plot(white_noise, main="White Noise with mu = 0 and sigma = 1")
abline(h=c(-1,0,1), lty=2)
```

## Simulate a WN model with $\mu$=100 and $\sigma$=10
```{r}
white_noise_2 <- arima.sim(model = list(order=c(0,0,0)), n = 100, mean = 100, sd = 10)
plot(white_noise_2, main="White Noise with mu = 100 and sigma = 10")
abline(h=c(90,100,110), lty=2)
```

## Estimate WN model and compare sample mean and variance
```{r}
arima(white_noise_2, order=c(0,0,0))
```
The estimate mean sample mean is exactly the sample mean and the estimate variance variance approximates the sample variance.
```{r}
mean(white_noise_2)
var(white_noise_2)
```

# Random Walk (RW)

Random Walk (RW) is a simple example of an unstable or non-stationary process.

A random walk has:

* No specified mean or variance  
* Strong dependence over time, with each observation closely related to its imediate neighbors
* Its changes or increments are white noise (WN) which is stable and stationary

**Randow Walk recursion:**

$$Today = Yesterday + Noise$$

More formally:

$$Y_t = Y_{t-1} + \epsilon_t$$

This error $\epsilon_t$ is *mean zero white noise* (WN)

* Simulating RW requires an initial point $Y_0$  
* Given $Y_0$ and the first WN term it's generated the first RW observation
* And then proceed with the next time index
* Often the value zero is used as initial point $Y_0$ for simplicity
* The RW model has only one parameter, the WN variance $\sigma_\epsilon^2$

**Random Walk process::**

The terms in RW can be arrange as a *fist difference series*

$$Y_t - Y_{t-1} = \epsilon_t \rightarrow diff(Y) \ is \ WN$$

## Simulate the random walk model

The random walk (RW) model is also a basic time series model. It is the cumulative sum (or integration) of a mean zero white noise (WN) series, such that the first difference series of a RW is a WN series. Note for reference that the RW model is an ARIMA(0, 1, 0) model, in which the middle entry of 1 indicates that the model's order of integration is 1.

The arima.sim() function can be used to simulate data from the RW by including the model = list(order = c(0, 1, 0)) argument. We also need to specify a series length n. Finally, you can specify a sd for the series (increments), where the default value is 1.

```{r}
random_walk <- arima.sim(model = list(order=c(0,1,0)), n = 100)
plot(random_walk, main="Random Walk")
```

## Calculate the first difference series of the RW model

The first difference of your random_walk data is white noise data. This is because a random walk is simply recursive white noise data. By removing the long-term trend, you end up with simple white noise.

```{r}
random_walk_diff <- diff(random_walk)
plot(random_walk_diff, main="First difference of the Random Walk model")
abline(h=c(-1,0,1), lty=2)
```

## Randow Walk with a Drift

The Random Walk model can be extended to include an Intercept or Drift Coefficient.  This adds a constant $c$ to the recursive formula:

$$Today = Constant + Yesterday + Noise$$

More formally:

$$Y_t = c + Y_{t-1} + \epsilon_t$$

This error $\epsilon_t$ is *mean zero white noise* (WN)

* The RW model has two parameters, constant $c$ and the WN variance $\sigma_\epsilon^2$

The first difference of a RW with a Drift is simply a constant + Noise

$$Y_t - Y_{t-1} = c + \epsilon_t \rightarrow WN \ with \ mean \ c$$

### Simulate the random walk model with a drift, e.g., $\mu$=1

A random walk (RW) need not wander about zero, it can have an upward or downward trajectory, i.e., a drift or time trend. This is done by including an intercept in the RW model, which corresponds to the slope of the RW time trend.

For an alternative formulation, you can take the cumulative sum of a constant mean white noise (WN) series, such that the mean corresponds to the slope of the RW time trend.

To simulate data from the RW model with a drift you again use the arima.sim() function with the model = list(order = c(0, 1, 0)) argument. This time, you should add the additional argument mean = ... to specify the drift variable, or the intercept.

```{r}
rw_drift <- arima.sim(model = list(order=c(0,1,0)), n = 100, mean = 1)
plot(rw_drift, main="Random Walk with a Drift, e.g., mu = 1")
```

### Calculate the first difference series of a RW model with a Drift
```{r}
rw_drift_diff <- diff(rw_drift,1)
plot(rw_drift_diff, main="First difference of the Random Walk model with a Drift")
abline(h=c(0,1,2), lty=2)
```

## Estimate the random walk model

For a given time series `y` we can fit the random walk model with a drift by first differencing the data, then fitting the white noise (WN) model to the differenced data using the `arima()` command with the `order = c(0, 0, 0))` argument.

The `arima()` command displays information or output about the fitted model. Under the `Coefficients:` heading is the estimated drift variable, named the `intercept`. Its approximate standard error (or s.e.) is provided directly below it. The variance of the WN part of the model is also estimated under the label `sigma^2`.

Given a Randow Noise model...
```{r}
rw_drift <- arima.sim(model = list(order=c(0,1,0)), n = 100, mean = 1)
plot(rw_drift, main="Randow Walk with a Drift")
```

...first difference the data...
```{r}
rw_drift_diff <- diff(rw_drift)
plot(rw_drift_diff)
abline(h=c(0,1,2), lty=2)
```

...fit the White Noise model to the differenced data to find the intercept...
```{r}
model_wn <- arima(rw_drift_diff, order=c(0,0,0))
model_wn$coef
```

...plot a time trend line with slope=0 and intercept from the WN model
```{r}
plot(rw_drift, main="Randow Walk with a Drift")
abline(a=0, b=model_wn$coef, col=2)
```

# Stationary Processes

## Stationarity

To obtain parsimony in a time series model you often assume some form of distributional invariance over time, or, stationarity

* Stationary models are parsimonious
* Stationary processes have distributional stability over time

For observed time series:

* Fluctuation appears random
* But behave similarly from one time period to the next

For example, returns in stocks or changes in interest rates each has very different behaviour from the previous year but their mean, standard deviation and other statistical properties are often similar from one year to the next

## Weak Stationarity

A process $Y_1,Y_2,...$ is *weakly stationary* process if its mean, variance and covariance are unchanged by time shifts:

* Mean $\mu$ of $Y_t$ is same (constant) for all $t$
* Variance $\sigma^2$ of $Y_t$ is same (constant) for all $t$
* Covariance of $Y_t$ and $Y_s$ is same (constant) for all $|t-s|=h$, for all $h$

The covariance between $Y$ at time $t$ and time $s$ only depends on how close $t$ and $s$ are and not at time indexes $t$ and $s$ themselves

For example, if the process is weakly stationary, $$Cov(Y_2, Y_5)=Cov(Y_7,Y_{10})$$ since each pair is separated by three units of time.

## Why focus on Stationarity Models?

A stationary process can be modeled with many fewer parameters.

For example, we do not need a different expectation (mean) for each observation $Y_t$;
rather they all have a common expectation (mean) $\mu$.

Estimate $\mu$ accurately by $\bar y$

When a time series is observed, a natural question is:

>Is it stationary?  Especifically, is a stationary process model appropriated for this time series data?

Many financial time series do not exhibit stationarity, however:

* The changes in the series are often approximately stationary, perhaps after aplying a log transformation.

* A stationary series should show random oscillation around some fixed level; a phenomenon called **mean-reversion**

## Are the white noise model or the random walk model stationary?

The white noise (WN) and random walk (RW) models are very closely related. However, only the RW is always non-stationary, both with and without a drift term.

Recall that if we start with a mean zero WN process and compute its running or cumulative sum, the result is a RW process. The cumsum() function will make this transformation for you. Similarly, if we create a WN process, but change its mean from zero, and then compute its cumulative sum, the result is a RW process with a drift.

```{r}
wn <- arima.sim(model=list(order=c(0,0,0)), n=100)
rw <- cumsum(white_noise)
wn_drift <- arima.sim(model=list(order=c(0,0,0)), n=100, mean=0.4)
rw_drift <- cumsum(wn_drift)
plot.ts(cbind(wn, rw, wn_drift, rw_drift), main="WN and RN with and without drift")
```

# Scatterplots

## Asset prices vs. asset returns

The goal of investing is to make a profit. The revenue or loss from investing depends on the amount invested and changes in prices, and high revenue relative to the size of an investment is of central interest. This is what financial asset returns measure, changes in price as a fraction of the initial price over a given time horizon, for example, one business day.

Let's again consider the eu_stocks dataset. This dataset reports index values, which we can regard as prices. The indices are not investable assets themselves, but there are many investable financial assets that closely track major market indices, including mutual funds and exchange traded funds.

Log returns, also called continuously compounded returns, are also commonly used in financial time series analysis. They are the log of gross returns, or equivalently, the changes (or first differences) in the logarithm of prices.

The change in appearance between daily prices and daily returns is typically substantial, while the difference between daily returns and log returns is usually small. As you'll see later, one advantage of using log returns is that calculating multi-period returns from individual periods is greatly simplified - you just add them together!

Plotting prices
```{r}
data(EuStockMarkets)
plot(EuStockMarkets)
```

Converting prices to (net) returns
```{r}
returns <- EuStockMarkets[-1,] / EuStockMarkets[-1860,] - 1
returns <- ts(returns, start = c(1991, 130), frequency = 260)
plot(returns)
```

Converting prices to log returns.
```{r}
logreturns <- diff(log(EuStockMarkets))
plot(logreturns)
```

## Characteristics of financial time series

Daily financial asset returns typically share many characteristics. Returns over one day are typically small, and their average is close to zero. At the same time, their variances and standard deviations can be relatively large. Over the course of a few years, several very large returns (in magnitude) are typically observed. These relative outliers happen on only a handful of days, but they account for the most substantial movements in asset prices. Because of these extreme returns, the distribution of daily asset returns is not normal, but heavy-tailed, and sometimes skewed. In general, individual stock returns typically have even greater variability and more extreme observations than index returns.

The eu_percentreturns dataset is the percentage returns calculated from your EuStockMarkets data.

```{r}
eu_percentreturns <- pch(EuStockMarkets)
plot(eu_percentreturns)
```

For each of the four indices contained in the data, it's calculated sample statistics mean, variance and standard deviation:

Sample mean
```{r}
colMeans(eu_percentreturns)
```

Sample variances
```{r}
apply(eu_percentreturns, MARGIN = 2, FUN = var)
```

Sample standard deviations
```{r}
apply(eu_percentreturns, MARGIN=2, FUN=sd)
```

Histogram of percent returns for each index
```{r}
par(mfrow = c(2,2))
invisible(apply(eu_percentreturns, MARGIN = 2, FUN = hist, main = "", xlab = "Percentage Return"))
```

Normal quantile plots of percent returns for each index
```{r}
par(mfrow = c(2,2))
invisible(apply(eu_percentreturns, MARGIN = 2, FUN = qqnorm, main = ""))
qqline(eu_percentreturns)
```

## Plotting pairs of data

Time series data is often presented in a time series plot. For example, the index values from the eu_stocks dataset are shown in the adjoining figure. Recall, eu_stocks contains daily closing prices from 1991-1998 for the major stock indices in Germany (DAX), Switzerland (SMI), France (CAC), and the UK (FTSE).

It is also useful to examine the bivariate relationship between pairs of time series. In this exercise we will consider the contemporaneous relationship, that is matching observations that occur at the same time, between pairs of index values as well as their log returns. The plot(a, b) function will produce a scatterplot when two time series names a and b are given as input.

To simultaneously make scatterplots for all pairs of several assets the pairs() function can be applied to produce a scatterplot matrix. When shared time trends are present in prices or index values it is common to instead compare their returns or log returns.

In this exercise, you'll practice these skills on the eu_stocks data. Because the DAX and FTSE returns have similar time coverage, you can easily make a scatterplot of these indices. Note that the normal distribution has elliptical contours of equal probability, and pairs of data drawn from the multivariate normal distribution form a roughly elliptically shaped point cloud. Do any of the pairs in the scatterplot matrices exhibit this pattern, before or after log transformation?

Scatterplot matrix of eu_stocks
```{r}
pairs(EuStockMarkets)
```

Scatterplot matrix of logreturns
```{r}
logreturns <- diff(log(EuStockMarkets))
pairs(logreturns)
```

# Covariance and Correlation

Covariance depends on the scale of the variables so it's hard to intepret whether a value is large.

Correlation is a standardized version of covariance that doesn't depend on the scale of the variables $$cor(x,y) = cov(x,y) / (\sigma_x \times \sigma_y)$$

Sample covariances measure the strength of the linear relationship between matched pairs of variables. The cov() function can be used to calculate covariances for a pair of variables, or a covariance matrix when a matrix containing several variables is given as input. For the latter case, the matrix is symmetric with covariances between variables on the off-diagonal and variances of the variables along the diagonal. On the right you can see the scatterplot matrix of your logreturns data.

Covariances are very important throughout finance, but they are not scale free and they can be difficult to directly interpret. Correlation is the standardized version of covariance that ranges in value from -1 to 1, where values close to 1 in magnitude indicate a strong linear relationship between pairs of variables. The cor() function can be applied to both pairs of variables as well as a matrix containing several variables, and the output is interpreted analogously.

Covariance of DAX and FTSE log returns
```{r}
cov(logreturns[, "DAX"], logreturns[, "FTSE"])
```
Covariance matrix of log returns
```{r}
cov(logreturns)
```
Correlation of DAX and FTSE log returns
```{r}
cor(logreturns[, "DAX"], logreturns[, "FTSE"])
```
Correlation matrix of log returns
```{r}
cor(logreturns)
```

# Autocorrelation

Autocorrelation is a very power tools for time series analysis. It's up to study how each time series observation is related to its recent past.  Processes with greatest autocorrelation are more predictable than those with don't.

## Lag 1 Autocorrelation

Compare each observation with it's preceeding one
```{r}
rw_series <- arima.sim(model = list(order=c(0,1,0)), n = 100, mean=0, sd=3)
cor(rw_series[-length(rw_series)], rw_series[-1])
```
```{r}
par(mfrow=c(1,2))
plot(rw_series, main="Stock prices", ylab="Price")
t <- length(rw_series)
plot(rw_series[-t], rw_series[-1], xlab=expression("Price"[t-1]),
     ylab=expression("Price"[t]), main="Lag 1 Scatterplot")
abline(a=0,b=cor(rw_series[-t], rw_series[-1]))
```

## Lag 2 Autocorrelation

```{r}
par(mfrow=c(1,2))
plot(rw_series, main="Stock prices", ylab="Price")
t <- length(rw_series)
plot(rw_series[-((t-1):t)], rw_series[-(1:2)], xlab=expression("Price"[t-2]),
     ylab=expression("Price"[t]), main="Lag 2 Scatterplot")
abline(a=0,b=cor(rw_series[-((t-1):t)], rw_series[-(1:2)]))
```

## Autocorrelation Function (acf) estimates for several lags simultaneously

Correlation estimates
```{r}
acf(rw_series, plot=FALSE)
```

Correlation plots

In this case the correlation becomes weaker as lags increases.  Each observation is positively associated with its predecessors until 17 lags but only the 12 first lags are statisticaly significant. The correlations become negative from lag 18 on.
```{r}
par(mar=c(4,4,4,1))
acf(rw_series)
```

# Autoregression (AR)

There are many autoregressive (AR) processes.  The simplest is the first order case in which today's observation is regressed on yesterday's observation at all times $t$.

The Autoregressive Model

The Autoregressive (AR) recursion:

$$Today = Constant + Slope \times Yesterday + Noise$$

Mean centered version:

$$(Today − Mean) = Slope \times (Y esterday − Mean) + Noise$$

More formally is $\epsilon$ is a mean zero white noise process than $Y$ is an AR process: $$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$

where $\epsilon_t$ is mean zero white noise (WN)

Three parameters:

* The mean $\mu$
* The slope $\phi$ (phi)
* The **WN** variance $\sigma_\epsilon^2$

If slope $\phi=0$ then $Y_t = \mu + \epsilon_t$ and $Y_t$ is White Noise $(\mu,\sigma_\epsilon^2)$.

If slope $\phi \neq 0$ then $Y_t$ depends on both $\epsilon_T$ and $Y_{t-1}$ and the process is autocorrelated.

Larger values of slope $\phi$ lead to greater autocorrelation.

Negative values of slope $\phi$ result in oscillatory time series.

if $\mu=0$ and slope $\phi=1$ then $$Y_t = Y_{t-1} + \epsilon_t$$ which is $$Today = Yesterday + Noise$$ which is a Random Walk (RW) process and $Y_t$ is not stationary in this case.

## Simulate the autoregressive model

The autoregressive (AR) model is arguably the most widely used time series model. It shares the very familiar interpretation of a simple linear regression, but here each observation is regressed on the previous observation (lag 1). The AR model also includes the white noise (WN) and random walk (RW) models examined in earlier chapters as special cases.

The versatile `arima.sim()` function used in previous chapters can also be used to simulate data from an AR model by setting the `model` argument equal to `list(ar = phi)`, in which `phi` is a slope parameter from the interval (-1, 1). We also need to specify a series length `n`.

This is the plot of three different AR models with slope parameters equal to 0.5, 0.9, and -0.75, respectively.

```{r}
x <- arima.sim(model = list(ar=0.5), n = 100)
y <- arima.sim(model = list(ar=0.9), n = 100)
z <- arima.sim(model = list(ar=-0.75), n = 100)
plot.ts(cbind(x, y, z))
```

As you can see, your `x` data shows a just a moderate amount of autocorrelation due to its moderate slope `phi` while your `y` data shows a large amount of autocorrelation due to its large slope `phi`. Alternatively, your z data tends to oscillate considerably from one observation to the next due negative moderate-to-high slope `phi`.

##Estimate the autocorrelation function (ACF) for an autoregression

What if you need to estimate the autocorrelation function from your data? To do so, you'll need the acf() command, which estimates autocorrelation by exploring lags in your data. By default, this command generates a plot of the relationship between the current observation and lags extending backwards.

These are the estimates of the autocorrelation function for the last threee simulated AR series (x, y, and z). These objects have slope parameters 0.5, 0.9, and -0.75, respectively:

```{r}
par(mfrow=c(1,3), mar=c(4,4,4,1))
acf(x); acf(y); acf(z)
```

The first series `x` has positive autocorrelation for the first couple lags, but they quickly approach zero. The second series `y` has positive autocorrelation for many lags, but they also decay to zero. The last series `z` has an alternating pattern, as does its autocorrelation function (ACF), but its ACF still quickly decays to zero in magnitude.

## Compare the random walk (RW) and autoregressive (AR) models

The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope parameter is equal to `1`. Recall from previous chapters that the RW model is not stationary and exhibits very strong persistence. Its sample autocovariance function (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values.

The stationary AR model has a slope parameter between `-1` and `1`. The AR model exhibits higher persistence when its slope parameter is closer to `1`, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate, indicating that values far in the past have little impact on future values of the process.

### Simulate and plot AR model with slope 0.9 
```{r}
par(mfrow=c(2,1), mar=c(4,4,1,1))
x <- arima.sim(model = list(ar=.9), n = 200)
ts.plot(x)
acf(x)
```

### Simulate and plot AR model with slope 0.98
```{r}
par(mfrow=c(2,1), mar=c(4,4,1,1))
y <- arima.sim(model = list(ar=0.98), n=200)
ts.plot(y)
acf(y)
```

### Simulate and plot RW model
```{r}
par(mfrow=c(2,1), mar=c(4,4,1,1))
z <- arima.sim(model = list(order=c(0,1,0)), n=200)
ts.plot(z)
acf(z)
```

As you can see, the AR model represented by series y exhibits greater persistence than series x, but the ACF continues to decay to 0. By contrast, the RW model represented by series z shows considerable persistence and relatively little decay in the ACF.

## AR model estimation and forescasting

### AR process: Inflation Rate

* One-month US inflation rate (in percent, annual rate)  
* Monthly observations from 1950 through 1990
* The one-month inflation rate is the first column

```{r}
data(Mishkin, package = "Ecdat")
inflation <- as.ts(Mishkin[, 1])
par(mfrow=c(2,1), mar=c(4,4,1,1))
ts.plot(inflation) ; acf(inflation)
```

From the time series plot one can see the inflation rate is usually positively valued and is fairly persistent, with and extend period of higher inflation beginning at the 1970's. From the ACF plot one can see strong, positive but decaying autocorrelation estimates from lags 1 to 26.  The AR model may provide a good fit to this data.


### AR Model: Inflation Rate

Recall the AR model:

$$(Today − Mean) = Slope \times (Y esterday − Mean) + Noise$$
$$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$
$$\epsilon_t \sim WhiteNoise(0,\sigma_\epsilon^2)$$

Applying ARIMA(1,0,0), a first order AR model, to the series inflation:

```{r}
AR_inflation <- arima(inflation, order = c(1, 0, 0))
AR_inflation
```

* The `ar1` coefficient is the estimate of the Slope $\phi$
```{r}
AR_inflation$coef[1]
```
* The `intercept` is the estimate of the Mean $\mu$
```{r}
AR_inflation$coef[2]
```
* The `sigma^2` is the estimate of the White Noise $\sigma_epsilon^2$
```{r}
AR_inflation$sigma2
```
* Approximate standard errors `s.e.` appear bellow the coefficients $\phi$ `ar1` and $\mu$ `intercept`.

### AR process: fitted values

* Estimates of $Today$ given $Yesterday$:

$$\widehat {Today} = \widehat {Mean} + \widehat {Slope} \times (Yesterday - \widehat {Mean})$$
more formally:

$$\hat Y_t =\hat\mu + \hat\phi(Y_{t-1} - \mu)$$

* Residuals are the observed values minus fitted values.  They are estimates of the White Noise.

$$Residuals = Today - \widehat {Today}$$

more formally:
$$\hat\epsilon_t = Y_t - \hat Y_t$$

The observed values are in black and the fitted values are in red.
```{r}
ts.plot(inflation)
AR_inflation_fitted <- inflation - residuals(AR_inflation)
points(AR_inflation_fitted, type = "l", col = "red", lty = 2)
```

### Forecasting

#### Inflation Rate
1-step ahead forecasts
```{r}
predict(AR_inflation)
```
h-steps ahead forecasts
```{r}
predict(AR_inflation, n.ahead=6)
```

#### Air Passengers
```{r}
AR <- arima(AirPassengers, order=c(1,0,0))
print(AR)
```
Plot the AirPassengers series and fitted values
```{r}
ts.plot(AirPassengers)
AR_fitted <- AirPassengers - residuals(AR)
points(AR_fitted, type = "l", col = 2, lty = 2)
```

#### The flow of River Nile

Forecasts using an AR model applied to the Nile data, which records annual observations of the flow of the River Nile from 1871 to 1970.

Fit an AR model to Nile
```{r}
AR_fit <-arima(Nile, order  = c(1,0,0))
AR_fit
```

Predict 1-step forecast
```{r}
predict_AR <- predict(AR_fit)
predict_AR$pred[1]
```

Predict 1-step through 10-steps forecasts
```{r}
predict(AR_fit, n.ahead = 10)
```

Plot the Nile series plus the forecast and 95% prediction intervals
```{r}
ts.plot(Nile, xlim = c(1871, 1980))
AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)
```

The predictions of River Nile flow from 1971 to 1980 make sense based on the data available. The relatively wide band of confidence (represented by the dotted lines) is a result of the low persistence in the Nile data.

# Simple Moving Average (MA)

## The Simple Moving Average (MA) model

Models with autocorrelation can be constructed from White Noise (WN).

A weighted sum of current and previous noise is called the Simple Moving Average process.

There are many moving average (MA) processess.  Here the focus is the simplest first order case in which today's observation is regressed on yesterday's noise.

If $\epsilon_t$ is mean zero white noise (WN) then $Y_t$ is an MA process that follows the equations:

$$Today = Mean + Noise + Slope \times Yesterday's\ Noise$$

More formally: $Y_t = \mu + \epsilon_t + \phi\epsilon_{t-1}$

Where $\epsilon_t$ is mean zero white noise (WN)

Three parameters:

The mean $\mu$
The slope $\theta$
The WN variance $\sigma_\epsilon^2$

## The MA process

If slope $\theta=0$ then $Y_t = \mu + \epsilon_t$ and $Y_t$ is White Noise $(\mu, \sigma_\epsilon^2)$

If slope $\theta \neq 0$ then $Y_t$ depends on both previous $\epsilon_{t-1}$ and current $\epsilon_T$ noise, and the process is autocorrelated.

Larger values of slope $\theta$ lead to greater autocorrelation.

Negative values of slope $\theta$ result in oscillatory time series.

## Autocorrelation

The MA process has autocorrelation is determined by the slope parameter $\theta$.  However, it only has dependence for one period, i.e., Only lag-1 autocorrelation non-zero for the MA model, i.e., the autocorrelation is zero for lags 2 and higher.

if $\theta$ is positive, the lag 1 autocorrelation is positive.

if $\theta$ is negative, the lag 1 autocorrelation is negative.

## Simulate the simple moving average model

The simple moving average (MA) model is a parsimonious time series model used to account for very short-run autocorrelation. It does have a regression like form, but here each observation is regressed on the previous innovation (WN variance), which is not actually observed. Like the autoregressive (AR) model, the MA model includes the white noise (WN) model as special case.

As with previous models, the MA model can be simulated using the arima.sim() command by setting the `model` argument to `list(ma = theta)`, where `theta` is a slope parameter from the interval `(-1, 1)`. Once again, you also need to specifcy the series length using the `n` argument.

Plots of the simulation of three MA models with slope parameters 0.5, 0.9, and -0.5, respectively.

```{r}
x <- arima.sim(model = list(ma=0.5), n = 100)
y <- arima.sim(model = list(ma=0.9), n=100)
z <- arima.sim(model=list(ma=-.5), n=100)
plot.ts(cbind(x,y,z), main="MA models with slope parameters 0.5, 0.9 and -0.5")
```

Note that there is some very short-run persistence for the positive slope values (x and y), and the series has a tendency to alternate when the slope value is negative (z).

## Estimate the autocorrelation function (ACF) for a moving average

Now that you've simulated some MA data using the arima.sim() command, you may want to estimate the autocorrelation functions (ACF) for your data. As in the previous chapter, you can use the acf() command to generate plots of the autocorrelation in your MA data.

In this exercise, you'll use acf() to estimate the ACF for three simulated MA series, x, y, and z. 
```{r}
par(mfrow=c(1,3), mar=c(4,4,4,1))
acf(x)
acf(y)
acf(z)
```

As you can see from your ACF plots, the series x has positive sample autocorrelation at the first lag, but it is approximately zero at other lags. The series y has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. The series z has an alternating pattern, and its sample autocorrelation is negative at the first lag. However, similar to the others, it is approximately zero for all higher lags.

## MA Model Estimation and Forecasting

### MA process: Inflation Change

* One-month US inflation rate (in percent, annual rate)  
* Monthly observations from 1950 through 1990  
* The one-month inflation rate is the first  
* The inflation_changes are the changes in one-month US inflation rate.  
* The inflation series wander around while the inflation changes quickly reverted to the mean.

```{r}
data(Mishkin, package = "Ecdat")
inflation <- as.ts(Mishkin[, 1])
inflation_changes <- diff(inflation)
par(mfrow=c(2,1), mar=c(4,4,1,1))
ts.plot(inflation) ; ts.plot(inflation_changes)
```

Inflation changtes and its sample ACF:

The inflation changes series is centered around zero and has an alternating pattern quickly changing from positive to negative.

The ACF plot strong negative autocorrelation estimate at lag 1 while the other autocorrelation estimates at lags 2 to 24 are all near zero.

The MA model may provid a good fit to this data.

```{r}
par(mfrow=c(1,2), mar=c(4,4,1,1))
ts.plot(inflation_changes)
acf(inflation_changes, lag.max = 24)
```

### MA model: Inflation Change

Recall the MA model:

$$Today = Mean + Noise + Slope \times Yesterday's\ Noise$$

$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$

$$\epsilon_t \sim WhiteNoise(0,\sigma_\epsilon^2)$$

Applying ARIMA(0,0,1), a first order MA model, to the series inflation changes:

```{r}
MA_inflation_changes <- arima(inflation_changes, order = c(0,0,1))
MA_inflation_changes
```

* The `ar1` coefficient is the estimate of the Slope $\theta$
```{r}
MA_inflation_changes$coef[1]
```
* The `intercept` is the estimate of the Mean $\mu$
```{r}
MA_inflation_changes$coef[2]
```
* The `sigma^2` is the estimate of the White Noise $\sigma_epsilon^2$
```{r}
MA_inflation_changes$sigma2
```
* Approximate standard errors `s.e.` appear bellow the coefficients $\theta$ `ar1` and $\mu$ `intercept`.

### MA process: fitted values

* Estimates of $Today$ given $Yesterday$:

$$\widehat {Today} = \widehat {Mean} + \widehat {Slope} \times \widehat {Yesterday's Noise}$$
more formally:

$$\hat Y_t =\hat\mu + \hat\theta \hat{\epsilon_{t-1}})$$

* Residuals are the observed values minus fitted values.  They are estimates of the White Noise.

$$Residuals = Today - \widehat {Today}$$

more formally:
$$\hat\epsilon_t = Y_t - \hat Y_t$$

The observed values are in black and the fitted values are in red.

There's a close relationship between the observed and fitted values.

The MA model explains a lot of the variation observed in this time series.

More advanced models may offer an improvment but the simple MA model does fairly well in this example.
```{r}
ts.plot(inflation_changes)
MA_inflation_changes_fitted <- inflation_changes - residuals(MA_inflation_changes)
points(MA_inflation_changes_fitted, type = "l", col = "red", lty = 2)
```

### Estimate the simple MA model: the flow of River Nile

MA model is an ARIMA(0, 0, 1) model.

```{r}
MA <- arima(Nile, order = c(0,0,1))
print(MA)
```

By fitting an MA model to the Nile data, it's capture the variation in the data for future prediction. Based on the plot generated, the MA model **does not appear to be a strong fit** for the Nile data.

```{r}
ts.plot(Nile)
MA_fit <- Nile - resid(MA)
points(MA_fit, type = "l", col = 2, lty = 1)
```

### Forecasting

#### Inflation changes

1-step ahead forecasts:

```{r}
predict(MA_inflation_changes)
```

h-step ahead forecasts:

```{r}
predict(MA_inflation_changes, n.ahead=6)
```

The 2 to 6 step are all exactly equal to 0.001 because the MA model only has autocorrelation for 1 time lag.

#### Simple forecasts from an estimated MA model

After estimated a MA model with the Nile data, the next step is to do some simple forecasting with the model. As with other types of models, it can use the `predict()` function to make simple forecasts from your estimated MA model. Recall that the `$pred` value is the forecast, while the `$se` value is a standard error for that forecast, each of which is based on the fitted MA model.

Once again, to make predictions for several periods beyond the last observation you can use the `n.ahead = h` argument in your call to predict(). The forecasts are made recursively from 1 to h-steps ahead from the end of the observed time series. **However, note that except for the 1-step forecast, all forecasts from the MA model are equal to the estimated mean (intercept)**.

1-step forecast based on MA
```{r}
predict_MA <- predict(MA)
predict_MA
```

1-step through 10-step forecast based on MA
```{r}
predict(MA, n.ahead=10)
```

#### Plot the Nile series plus the forecast and 95% prediction intervals
```{r}
MA_forecast <- predict(MA, n.ahead = 10)$pred
MA_forecast_se <- predict(MA, n.ahead = 10)$se
ts.plot(Nile, xlim = c(1871, 1980))
points(MA_forecast, type = "l", col = 2)
points(MA_forecast - 2*MA_forecast_se, type = "l", col = 2, lty = 2)
points(MA_forecast + 2*MA_forecast_se, type = "l", col = 2, lty = 2)
```

Note that the MA model can only produce a 1-step forecast. For additional forecasting periods, the predict() command simply extends the original 1-step forecast. This explains the unexpected horizontal lines after 1971.

# Compare the MA and AR models

The MA and AR models are similar in many ways:

* Both have mean zero white noise (WN) terms with $\epsilon$ parameter squared  
* Both include a mean paramenter $\mu$
* In the MA model, Today's observation $Y_t$ is regressed on Yesterday's noise $\epsilon_{t-1}$
* In the AR model, Today's observation $Y_t$ is regressed on Yesterday's observation $Y_{t-1}$
* The MA model only has autocorrelation at 1 lag
* The AR model has autocorrelation at many lags

MA model:
$$Today = Mean + Noise + Slope \times Yesterday's\ Noise$$
$$Y_t = \mu + \epsilon_t + \theta\epsilon_{t-1}$$
AR model:
$$(Today − Mean) = Slope \times (Y esterday − Mean) + Noise$$
$$Y_t - \mu = \phi(Y_{t-1} - \mu) + \epsilon_t$$
Where:
$$\epsilon_t \sim WhiteNoise(0,\sigma_\epsilon^2)$$

## Simulated MA and AR models

```{r}
seed<-1 # set same seeds ensure same white noise
set.seed(seed);MA1 <- arima.sim(model = list(ma=0.75), n = 100)
set.seed(seed);MA2 <- arima.sim(model = list(ma=-0.75), n = 100)
set.seed(seed);AR1 <- arima.sim(model = list(ar=0.48), n = 100)
set.seed(seed);AR2 <- arima.sim(model = list(ar=-0.48), n = 100)
```

## Autocorrelations

```{r}
MA1_acf <- acf(MA1, plot=FALSE)
MA2_acf <- acf(MA2, plot=FALSE)
AR1_acf <- acf(AR1, plot=FALSE)
AR2_acf <- acf(AR2, plot=FALSE)
par(mfcol=c(2,2), mar=c(4,4,3,1))
plot(MA1_acf, ylab=expression(rho(h)), main=expression(paste("MA: ", theta, " = 0.75")))
lines(MA1_acf$lag, MA1_acf$acf, lty=3)
plot(MA2_acf, ylab=expression(rho(h)), main=expression(paste("MA: ", theta, " = -0.75")))
lines(MA2_acf$lag, MA2_acf$acf, lty=3)
plot(AR1_acf, ylab=expression(rho(h)), main=expression(paste("AR: ", phi, " = 0.48")))
lines(AR1_acf$lag, AR1_acf$acf, lty=3)
plot(AR2_acf, ylab=expression(rho(h)), main=expression(paste("AR: ", phi, " = -0.48")))
lines(AR2_acf$lag, AR2_acf$acf, lty=3)
```

The autocorrelation of the MA model with the slope $\theta=0.75$ on top left is similar to the autocorrelation of the AR model with slope $\phi=0.48$ on top right.

The autocorrelation of the MA model with slope $\theta=-0.75$ on bottom left is similar to the autocorrelation of the AR model with slope $\phi=-0.48$ on bottom right.

Remember, larger values of slope lead to greater autocorrelation.

Remember, negative values of slope result in oscillatory time series as one can see in the bottom figures.

```{r}
par(mfcol=c(2,2), mar=c(2,3,3,1))
plot(MA1, ylab="Value", main=expression(paste("MA: ", theta, "= 0.75")))
abline(h=c(-1,0,1), lty=3)
plot(MA2, ylab="Value", main=expression(paste("MA: ", theta, "= -0.75")))
abline(h=c(-1,0,1), lty=3)
plot(AR1, ylab="Value", main=expression(paste("AR: ", phi, "= 0.48")))
abline(h=c(-1,0,1), lty=3)
plot(AR2, ylab="Value", main=expression(paste("AR: ", phi, "= -0.48")))
abline(h=c(-1,0,1), lty=3)
```

The top models have similar lag-1 autocorrelation
```{r}
MA1_acf$acf[MA1_acf$lag==1]
AR1_acf$acf[AR1_acf$lag==1]
```

The bottom models also have similar lag-1 autocorrelation and more pronounced oscillating pattern due to their negative slopes.
```{r}
MA2_acf$acf[MA2_acf$lag==1]
AR2_acf$acf[AR2_acf$lag==1]
```

The MA model with $\theta=0.75$ is relative similar to the AR model with $\phi=0.48$.

The MA model with $\theta=-0.75$ is relative similar to the AR model with $\phi=-0.48$.

```{r}
par(mfcol=c(2,1), mar=c(2,3,3,1))
plot(MA1, ylab="Value", main="MA models", lwd=2)
abline(h=c(-1,0,1), lty=3)
lines(MA2, lwd=2, col=2)
legend("bottomleft", lty=1, col=1:2, text.col=1:2, cex=0.7, c(expression(paste("slope ", theta, " 0.75")), expression(paste("slope ", theta, " -0.75"))))
plot(AR1, ylab="Value", main="AR models", lwd=2)
abline(h=c(-1,0,1), lty=3)
lines(AR2, lwd=2, col=2)
legend("bottomleft", lty=1, col=1:2, text.col=1:2, cex=0.7, c(expression(paste("slope ", phi, " 0.48")), expression(paste("slope ", phi, " -0.48"))))
```

## MA and AR Processes: Fitted Values

Changes in one-month US inflation rate

```{r}
data(Mishkin, package = "Ecdat")
inflation <- as.ts(Mishkin[, 1])
#inflation <- window(inflation, c(1989,0), c(1991,0))
inflation_changes <- diff(inflation)
MA_inflation_changes <- arima(inflation_changes, order = c(0,0,1))
AR_inflation_changes <- arima(inflation_changes, order = c(1,0,0))
MA_inflation_changes_fitted <- inflation_changes - residuals(MA_inflation_changes)
AR_inflation_changes_fitted <- inflation_changes - residuals(AR_inflation_changes)
inflation_changes <- window(inflation_changes, c(1989,0), c(1991,0))
MA_inflation_changes_fitted <- window(MA_inflation_changes_fitted, c(1989,0), c(1991,0))
AR_inflation_changes_fitted <- window(AR_inflation_changes_fitted, c(1989,0), c(1991,0))
plot(inflation_changes, type="l", lty=3)
points(MA_inflation_changes_fitted, type = "l", col=2)
points(AR_inflation_changes_fitted, type = "l", col=4)
legend("topleft", legend=c("inflation changes", "MA fit", "AR fit"), col=c(1,2,4), text.col =c(1,2,4), lty=c(3,1,1))
```

Fitted values are quite similiar.

Simliar fits are expected when estimate lag-1 autocorrelation is bellow 0.5 and the other estimates of autocorrelation are small or zero.

Back testing or forecast updating are more advanced concepts that can be used to determine which of the two models produces better forecasts for a given dataset.

Another approach to selecting a model is using ae goodness of fit measure such as information criterion. Specifically the AIC (Akaike's An Information Criterion) and the BIC or SBC (Schwarz's Bayesian criterion) are commonly used for time series models.  Information Criterio is a more advanced concepts, but for either measure a lower value indicates a relatively better fitting model.

```{r}
AIC(MA_inflation_changes)
BIC(MA_inflation_changes)
AIC(AR_inflation_changes)
BIC(AR_inflation_changes)
```

The AIC of the MA model has the lower value.  The MA model provides a better fit to the inflation changes series.

### AR vs MA Models

Autoregressive (AR) and simple moving average (MA) are two useful approaches to modeling time series. But how can to determine whether an AR or MA model is more appropriate in practice?

To determine model fit, one can measure the Akaike information criterion (AIC) and Bayesian information criterion (BIC) for each model. While the math underlying the AIC and BIC is beyond the scope of this course, for this purposes the main idea is these these indicators penalize models with more estimated parameters, to avoid overfitting, and smaller values are preferred. All factors being equal, a model that produces a lower AIC or BIC than another model is considered a better fit.

To estimate these indicators, one can use the AIC() and BIC() commands, both of which require a single argument to specify the model in question.

```{r}
AR <- arima(Nile, order = c(1, 0, 0))
MA <- arima(Nile, order = c(0, 0, 1))
AR_fit <- Nile - residuals(AR)
MA_fit <- Nile - residuals(MA)

cor(AR_fit, MA_fit)
AIC(AR)
AIC(MA)
BIC(AR)
BIC(MA)
```

Although the predictions from both models are very similar (indeed, they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for the Nile data.

